{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12092882,"sourceType":"datasetVersion","datasetId":7612618},{"sourceId":12102032,"sourceType":"datasetVersion","datasetId":7618853}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install python-pptx\n\n# ======== 1. CORE DEPENDENCIES (versions pinned) ========\n!pip install --upgrade pip\n\n!pip install \\\n  keybert \\\n  PyMuPDF \\\n  transformers \\\n  python-docx \\\n  Pillow \\\n  nltk \\\n  spacy \\\n  en-core-web-sm \\\n  scikit-learn \\\n  torch \\\n  torchvision \\\n  sentence-transformers \\\n  sentencepiece \\\n  chromadb \\\n  langchain \\\n  pytesseract \\\n  pdfplumber \\\n  git+https://github.com/openai/CLIP.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:46:15.384001Z","iopub.execute_input":"2025-06-12T22:46:15.384311Z","iopub.status.idle":"2025-06-12T22:46:26.358306Z","shell.execute_reply.started":"2025-06-12T22:46:15.384284Z","shell.execute_reply":"2025-06-12T22:46:26.357234Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: python-pptx in /usr/local/lib/python3.11/dist-packages (1.0.2)\nRequirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (11.1.0)\nRequirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (3.2.3)\nRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.3.1)\nRequirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (4.13.2)\nRequirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ntzhj_t8\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ntzhj_t8\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: keybert in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\nRequirement already satisfied: en-core-web-sm in /usr/local/lib/python3.11/dist-packages (3.8.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.12)\nRequirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\nRequirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.26.4)\nRequirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (14.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->keybert) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->keybert) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->keybert) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->keybert) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->keybert) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->keybert) (2.4.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\nRequirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\nRequirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.8.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\nRequirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.55b1)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\nRequirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.72.0rc1)\nRequirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\nRequirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\nRequirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\nRequirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\nRequirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (4.9.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (1.3.1)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.50)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\nRequirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.23)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\nRequirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (44.0.3)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.40.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\nRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\nRequirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\nRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\nRequirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\nRequirement already satisfied: opentelemetry-instrumentation-asgi==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b1)\nRequirement already satisfied: opentelemetry-instrumentation==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b1)\nRequirement already satisfied: opentelemetry-util-http==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b1)\nRequirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\nRequirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\nRequirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\nRequirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\nRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->keybert) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->keybert) (2022.1.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.5->keybert) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->keybert) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.5->keybert) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Normal Model","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport io\nimport re\nimport fitz  # PyMuPDF\nimport docx\nfrom PIL import Image\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel, pipeline\nfrom keybert import KeyBERT\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize\nfrom google.colab import files\nimport chromadb\nfrom chromadb.utils import embedding_functions\nfrom typing import List, Dict, Union\nimport uuid\n\n\n# -------------------- Constants --------------------\n\nTEXT_MODEL_ID = 'paraphrase-multilingual-MiniLM-L12-v2'  # For KeyBERT only\nIMAGE_MODEL_ID = 'openai/clip-vit-base-patch32'\nTITLE_WORD_RANGE = (3, 12)\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n# -------------------- Document Extraction --------------------\n\nclass DocumentExtractor:\n    def __init__(self, file_path):\n        self.file_path = file_path\n        self.extension = os.path.splitext(file_path)[-1].lower()\n\n    def extract_text(self):\n        if self.extension == '.pdf':\n            return self._extract_text_from_pdf()\n        elif self.extension == '.docx':\n            return self._extract_text_from_docx()\n        else:\n            raise ValueError(\"Unsupported file type\")\n\n    def extract_images(self):\n        if self.extension == '.pdf':\n            return self._extract_images_from_pdf()\n        elif self.extension == '.docx':\n            return self._extract_images_from_docx()\n        else:\n            raise ValueError(\"Unsupported file type\")\n\n    def _extract_text_from_pdf(self):\n        text = \"\"\n        doc = fitz.open(self.file_path)\n        for page in doc:\n            text += page.get_text()\n        return text\n\n    def _extract_text_from_docx(self):\n        doc = docx.Document(self.file_path)\n        return \"\\n\".join([para.text for para in doc.paragraphs])\n\n    def _extract_images_from_pdf(self):\n        doc = fitz.open(self.file_path)\n        images = []\n        for page in doc:\n            for img in page.get_images(full=True):\n                try:\n                    xref = img[0]\n                    base_image = doc.extract_image(xref)\n                    image = Image.open(io.BytesIO(base_image[\"image\"])).convert(\"RGB\")\n                    images.append(image)\n                except Exception as e:\n                    print(f\"❌ PDF image error: {e}\")\n        return images\n\n    def _extract_images_from_docx(self):\n        doc = docx.Document(self.file_path)\n        images = []\n        for rel in doc.part.rels.values():\n            if \"image\" in rel.target_ref:\n                try:\n                    image = Image.open(io.BytesIO(rel.target_part.blob)).convert(\"RGB\")\n                    images.append(image)\n                except Exception as e:\n                    print(f\"❌ DOCX image error: {e}\")\n        return images\n\n\n# -------------------- Title Extraction --------------------\n\nclass TitleExtractor:\n    def __init__(self, lines=None, full_text=None, model_id=None,\n                 min_words=3, max_words=12,\n                 keyphrase_ngram_range=(2,4), stop_words='english'):\n        self.lines = lines or []\n        self.full_text = full_text or \"\"\n        self.model_id = model_id\n        self.model = KeyBERT(model=model_id) if model_id else KeyBERT()\n\n        self.min_words = min_words\n        self.max_words = max_words\n        self.keyphrase_ngram_range = keyphrase_ngram_range\n        self.stop_words = stop_words\n\n    def _filter_lines(self):\n        # Filter lines by length and initial uppercase letter\n        return [\n            line for line in self.lines\n            if self.min_words <= len(line.split()) <= self.max_words and re.match(r\"^[A-Z]\", line)\n        ]\n\n    def extract_candidate_titles(self):\n        filtered_lines = self._filter_lines()\n        # Batch extraction of keywords from lines\n        titles = set()\n        for line in filtered_lines:\n            keywords = self.model.extract_keywords(\n                line,\n                keyphrase_ngram_range=self.keyphrase_ngram_range,\n                stop_words=self.stop_words,\n                top_n=1\n            )\n            if keywords:\n                titles.add(keywords[0][0])\n        return list(titles)\n\n    def extract_semantic_titles(self, top_n=10):\n        sentences = sent_tokenize(self.full_text)\n        joined_text = \" \".join(sentences)\n        keywords = self.model.extract_keywords(\n            joined_text,\n            keyphrase_ngram_range=self.keyphrase_ngram_range,\n            stop_words=self.stop_words,\n            use_maxsum=True,\n            top_n=top_n\n        )\n        return [kw[0] for kw in keywords]\n\n\n# -------------------- CLIP Text Embedding --------------------\n\nclass CLIPTextEmbedder:\n    def __init__(self, model_id=IMAGE_MODEL_ID, device=DEVICE):\n        self.device = device\n        self.model = CLIPModel.from_pretrained(model_id).to(device)\n        self.processor = CLIPProcessor.from_pretrained(model_id)\n\n    def encode(self, texts, batch_size=32):\n        \"\"\"Encode text using CLIP text encoder\"\"\"\n        embeddings = []\n\n        # Process in batches to avoid memory issues\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i + batch_size]\n            inputs = self.processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n\n            with torch.no_grad():\n                text_features = self.model.get_text_features(**inputs)\n                text_features /= text_features.norm(p=2, dim=-1, keepdim=True)\n                embeddings.append(text_features.cpu().numpy())\n\n        return np.vstack(embeddings)\n\n\n# -------------------- Image Embedding --------------------\nclass ImageEmbedder:\n    def __init__(self, model_id=IMAGE_MODEL_ID, device=DEVICE):\n        self.device = device\n        self.model = CLIPModel.from_pretrained(model_id).to(device)\n        self.processor = CLIPProcessor.from_pretrained(model_id)\n\n    def encode(self, images):\n        embeddings = []\n        for img in images:\n            inputs = self.processor(images=img, return_tensors=\"pt\").to(self.device)\n            with torch.no_grad():\n                features = self.model.get_image_features(**inputs)\n                features /= features.norm(p=2, dim=-1, keepdim=True)\n                embeddings.append(features.cpu().numpy())\n        return np.vstack(embeddings)  # Stack the list of arrays into a single array\n\n\n#---------------Vector DB-------------------------------\n\nclass ChromaDBManager:\n    def __init__(self, persist_dir: str = \"./chroma_db\"):\n        self.client = chromadb.PersistentClient(path=persist_dir)\n\n        # Only need collections for content we'll retrieve\n        self.text_collection = self.client.get_or_create_collection(\n            name=\"document_texts\",\n            embedding_function=None  # We'll provide pre-computed embeddings\n        )\n        self.image_collection = self.client.get_or_create_collection(\n            name=\"document_images\",\n            embedding_function=None\n        )\n\n    def store_texts(self, texts: List[str], embeddings: np.ndarray) -> List[str]:\n        \"\"\"Store text embeddings\"\"\"\n        ids = [str(uuid.uuid4()) for _ in texts]\n        self.text_collection.add(\n            ids=ids,\n            documents=texts,\n            embeddings=embeddings.tolist()\n        )\n        return ids\n\n    def store_images(self, image_indices: List[int], embeddings: np.ndarray) -> List[str]:\n        \"\"\"Store image embeddings\"\"\"\n        ids = [str(uuid.uuid4()) for _ in image_indices]\n        self.image_collection.add(\n            ids=ids,\n            # uris=image_indices,\n            embeddings=embeddings.tolist(),\n            metadatas=[{\"index\": idx} for idx in image_indices]\n        )\n        return ids\n\n\n    def query_texts(self, query_embedding: np.ndarray, n_results: int = 5) -> List[str]:\n        \"\"\"Query similar texts\"\"\"\n        results = self.text_collection.query(\n            query_embeddings=query_embedding.tolist(),\n            n_results=n_results\n        )\n        return results['documents'][0] if results['documents'] else []\n\n    def query_images(self, query_embedding: np.ndarray, n_results: int = 3) -> List[int]:\n        \"\"\"Query similar images and return their indices\"\"\"\n        results = self.image_collection.query(\n            query_embeddings=query_embedding.tolist(),\n            n_results=n_results\n        )\n        if not results['metadatas']:\n            return []\n\n        # Extract indices from metadata\n        return [meta['index'] for meta in results['metadatas'][0] if 'index' in meta]\n\n# -------------------- Slide Builder --------------------\n\nclass SlideBuilder:\n    def __init__(self, titles, title_embeddings, lines, images,\n                 db_manager: ChromaDBManager,\n                 text_embedder: CLIPTextEmbedder,\n                 max_bullets_per_slide=5, overlap_bullets=1,\n                 summarization_model=\"facebook/bart-large-cnn\"):\n\n        self.titles = titles\n        self.title_embeddings = title_embeddings\n        self.lines = lines\n        self.images = images\n        self.db_manager = db_manager\n        self.max_bullets = max_bullets_per_slide\n        self.overlap_bullets = overlap_bullets\n        self.summarizer = pipeline(\"summarization\", model=summarization_model)\n        self.text_embedder = text_embedder\n\n    def _summarize_text(self, text):\n        \"\"\"Summarize text into bullet points\"\"\"\n        if not text or len(text.strip()) < 50:\n            return [text.strip()] if text.strip() else []\n\n        try:\n            max_input_length = 1024\n            if len(text) > max_input_length:\n                text = text[:max_input_length]\n\n            summary = self.summarizer(text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n            bullets = [b.strip() for b in re.split(r'[.;]\\s*', summary) if b.strip()]\n            return bullets if bullets else [summary]\n\n        except Exception as e:\n            print(f\"Summarization error: {e}\")\n            return sent_tokenize(text)[:self.max_bullets]\n\n    def build_slides(self):\n        \"\"\"Build slides using ChromaDB for all retrieval operations\"\"\"\n        slides = []\n        previous_bullets = []\n\n        for title, title_emb in zip(self.titles, self.title_embeddings):\n            # Query relevant text using title embedding\n            relevant_texts = self.db_manager.query_texts(title_emb, n_results=20)\n            if not relevant_texts:\n                slides.append({\n                    'title': title,\n                    'bullets': [f\"No relevant content found for: {title}\"],\n                    'images': []\n                })\n                continue\n\n            # Summarize the relevant text\n            combined_text = \" \".join(relevant_texts)\n            bullets = self._summarize_text(combined_text)\n\n            # Split into slides with overlap\n            start_idx = 0\n            while start_idx < len(bullets):\n              if start_idx > 0 and previous_bullets:\n                  overlap = previous_bullets[-self.overlap_bullets:]\n              else:\n                  overlap = []\n\n              end_idx = start_idx + self.max_bullets\n              slide_bullets = overlap + bullets[start_idx:end_idx]\n              slides.append({\n                  'title': title,\n                  'bullets': slide_bullets,\n                  'images': self._retrieve_images(title_emb)\n              })\n              previous_bullets = slide_bullets\n              start_idx = end_idx\n\n\n        return slides\n\n    def _retrieve_images(self, query_embedding):\n      image_indices = self.db_manager.query_images(query_embedding, n_results=3)\n      return [self.images[idx] for idx in image_indices if idx < len(self.images)]\n\n\n    def _load_image(self, image_index):\n        try:\n            if isinstance(image_index, int) and 0 <= image_index < len(self.images):\n                return self.images[image_index]\n        except Exception as e:\n            print(f\"Error loading image at index {image_index}: {e}\")\n        return None\n\n\n    def _image_exists(self, image_index):\n        \"\"\"Check if image index is valid\"\"\"\n        return isinstance(image_index, int) and 0 <= image_index < len(self.images)\n\n# -------------------- Pipeline Orchestration --------------------\nclass DocumentProcessingPipeline:\n    def __init__(self, file_path, use_semantic=True, persist_db: bool = True, top_titles=20):\n        self.file_path = file_path\n        self.extractor = DocumentExtractor(file_path)\n        self.text_embedder = CLIPTextEmbedder()\n        self.image_embedder = ImageEmbedder()\n        self.title_extractor = None\n        self.use_semantic = use_semantic\n        self.top_titles = top_titles\n        self.db_manager = ChromaDBManager() if persist_db else None\n\n    def run(self):\n        print(f\"Extracting content from {self.file_path}\")\n        raw_text = self.extractor.extract_text()\n        images = self.extractor.extract_images()\n        lines = [line.strip() for line in raw_text.split('\\n') if line.strip()]\n\n        self.title_extractor = TitleExtractor(lines=lines, full_text=raw_text)\n        titles = self.title_extractor.extract_semantic_titles(top_n=self.top_titles) if self.use_semantic else self.title_extractor.extract_candidate_titles()\n\n        # Embed all content\n        text_embeddings = self.text_embedder.encode(lines)\n        image_embeddings = self.image_embedder.encode(images) if images else []\n        title_embeddings = self.text_embedder.encode(titles)\n\n        # Store in ChromaDB if enabled\n        if self.db_manager:\n            self.db_manager.store_texts(lines, text_embeddings)\n            if images:\n                image_indices = list(range(len(images)))\n                self.db_manager.store_images(image_indices, image_embeddings)\n\n        return {\n            \"titles\": titles,\n            \"title_embeddings\": title_embeddings,\n            \"lines\": lines,\n            \"images\": images,\n            \"db_manager\": self.db_manager,\n            \"text_embeddings\": text_embeddings,\n            \"image_embeddings\": image_embeddings,\n            \"text_embedder\": self.text_embedder,\n            \"image_embedder\": self.image_embedder,\n            \"title_extractor\": self.title_extractor,\n            \"use_semantic\": self.use_semantic,\n            \"raw_text\": raw_text,\n        }","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# New Model","metadata":{}},{"cell_type":"markdown","source":"1. *Download Dependencies*","metadata":{}},{"cell_type":"markdown","source":"## spaCy & NLTK","metadata":{}},{"cell_type":"code","source":"# 1) spaCy model\n!python -m spacy download en_core_web_sm\n\n# 2) NLTK data\nimport nltk, os\nnltk_data_path = \"/content/nltk_data\"\nos.makedirs(nltk_data_path, exist_ok=True)\n\nfor pkg in [\"punkt\", \"stopwords\", \"wordnet\", \"averaged_perceptron_tagger\"]:\n    nltk.download(pkg, download_dir=nltk_data_path)\n\n# Tell NLTK to also look in that folder:\nnltk.data.path.append(nltk_data_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:46:26.360148Z","iopub.execute_input":"2025-06-12T22:46:26.360953Z","iopub.status.idle":"2025-06-12T22:46:34.645106Z","shell.execute_reply.started":"2025-06-12T22:46:26.360923Z","shell.execute_reply":"2025-06-12T22:46:34.644170Z"}},"outputs":[{"name":"stdout","text":"Collecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /content/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /content/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /content/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /content/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Important Imports","metadata":{}},{"cell_type":"code","source":"# === Standard Library ===\nimport os\nimport io\nimport uuid\nimport json\nimport re\nfrom typing import List, Dict, Tuple, Any, Union, Optional, Literal\nfrom dataclasses import dataclass\nimport numpy as np\nimport google.generativeai as genai\nimport warnings\n\n# === Third-Party Libraries ===\n\n# NLP and Transformers\nfrom transformers import (\n    CLIPProcessor,\n    CLIPModel,\n    BlipProcessor,\n    BlipForConditionalGeneration,\n    T5Tokenizer,\n    T5TokenizerFast,\n    T5ForConditionalGeneration,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    pipeline,\n)\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.tokenize import sent_tokenize\nfrom keybert import KeyBERT\n\n# Deep Learning and Vision\nimport torch\nimport clip\nfrom torchvision import transforms\nfrom PIL import Image\n\n# PDF, DOCX, and file handling\nimport fitz            # PyMuPDF\nimport docx            # python-docx\nimport pdfplumber\nimport pandas as pd    # For table handling\nfrom google.colab import files\n\n# Vector DB and text processing\nimport chromadb\nfrom chromadb.utils import embedding_functions\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport collections","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-06-12T22:46:34.646525Z","iopub.execute_input":"2025-06-12T22:46:34.646882Z","iopub.status.idle":"2025-06-12T22:46:46.822838Z","shell.execute_reply.started":"2025-06-12T22:46:34.646860Z","shell.execute_reply":"2025-06-12T22:46:46.821989Z"}},"outputs":[{"name":"stderr","text":"2025-06-12 22:46:40.985768: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749768401.008600     575 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749768401.015696     575 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### The Model","metadata":{}},{"cell_type":"code","source":"# -------------------- Constants --------------------\nTEXT_MODEL_ID = 'paraphrase-multilingual-MiniLM-L12-v2'  # For KeyBERT only\nIMAGE_MODEL_ID = 'openai/clip-vit-base-patch32'\nTITLE_WORD_RANGE = (3, 12)\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n# -------------------- Document Extraction --------------------\nclass DocumentExtractor:\n    def __init__(self):\n        self.supported_formats = {\n            '.pdf':  self.extract_from_pdf,\n            '.docx': self.extract_from_docx,\n        }\n\n    def extract(self, path: str) -> Tuple[List[Dict], List[Dict], List[Dict]]:\n        ext = os.path.splitext(path)[1].lower()\n        if ext not in self.supported_formats:\n            raise ValueError(f\"Unsupported format: {ext}\")\n        return self.supported_formats[ext](path)\n\n    def extract_from_pdf(self, pdf_path: str):\n        texts, images, tables = [], [], []\n        doc = fitz.open(pdf_path)\n\n        # 1a) text & images via PyMuPDF\n        for page_num, page in enumerate(doc, start=1):\n            texts.append({\n                'page': page_num,\n                'content': page.get_text() or \"\"\n            })\n            try:\n                for img_idx, img in enumerate(page.get_images(full=True), start=1):\n                    xref = img[0]\n                    pix = fitz.Pixmap(doc, xref)\n                    if pix.n - pix.alpha > 3:\n                        pix = fitz.Pixmap(fitz.csRGB, pix)\n                    data = pix.tobytes(\"png\")\n                    pil = Image.open(io.BytesIO(data))\n                    images.append({\n                        'page': page_num,\n                        'image_index': img_idx,\n                        'image': pil\n                    })\n            except Exception as e:\n                print(f\"Error extracting image on page {page_num}: {str(e)}\")\n                continue\n\n        doc.close()\n\n        # 1b) tables via pdfplumber\n        if pdfplumber:\n            with pdfplumber.open(pdf_path) as plumber:\n                for page_num, page in enumerate(plumber.pages, start=1):\n                    for tbl_idx, raw in enumerate(page.extract_tables(), start=1):\n                        tables.append({\n                            'page': page_num,\n                            'table_index': tbl_idx,\n                            'table': raw   # List[List[str]]\n                        })\n\n        return texts, images, tables\n\n    def extract_from_docx(self, docx_path: str):\n        doc = docx.Document(docx_path)\n        texts, images, tables = [], [], []\n\n        # 2a) paragraphs\n        for i, para in enumerate(doc.paragraphs, start=1):\n            texts.append({'paragraph': i, 'content': para.text})\n\n        # 2b) inline images\n        for i, shape in enumerate(doc.inline_shapes, start=1):\n            rid = shape._inline.graphic.graphicData.pic.blipFill.blip.embed\n            part = doc.part.related_parts[rid]\n            pil = Image.open(io.BytesIO(part.blob))\n            images.append({'paragraph': None, 'image_index': i, 'image': pil})\n\n        # 2c) tables\n        for t_i, table in enumerate(doc.tables, start=1):\n            rows = [[cell.text for cell in row.cells] for row in table.rows]\n            tables.append({\n                'table_index': t_i,\n                'table': rows\n            })\n\n        return texts, images, tables\n\n# -------------------- Title Extraction --------------------\nclass TitleExtractor:\n    def __init__(self, lines=None, full_text=None, model_id=None,\n                 min_words=3, max_words=12,\n                 keyphrase_ngram_range=(2,4), stop_words='english'):\n        self.lines = lines or []\n        self.full_text = full_text or \"\"\n        self.model_id = model_id\n        self.model = KeyBERT(model=model_id) if model_id else KeyBERT()\n\n        self.min_words = min_words\n        self.max_words = max_words\n        self.keyphrase_ngram_range = keyphrase_ngram_range\n        self.stop_words = stop_words\n\n    def extract_titles(self, option: Literal['fontbased', 'semantic', 'candidate'], pdf_path: str = None, top_n: int = 10) -> List[str]:\n        if option == 'fontbased':\n            if not pdf_path:\n                raise ValueError(\"pdf_path is required for fontbased title extraction.\")\n            return self.extract_titles_fontbased(pdf_path, top_n_fonts=2)\n        elif option == 'semantic':\n            return self.extract_semantic_titles(top_n=top_n)\n        elif option == 'candidate':\n            return self.extract_candidate_titles()\n        else:\n            raise ValueError(f\"Invalid title extraction option: {option}\")\n\n    def _filter_lines(self):\n        # Filter lines by length and initial uppercase letter\n        return [\n            line for line in self.lines\n            if self.min_words <= len(line.split()) <= self.max_words and re.match(r\"^[A-Z]\", line)\n        ]\n\n    def extract_candidate_titles(self):\n        filtered_lines = self._filter_lines()\n        # Batch extraction of keywords from lines\n        titles = set()\n        for line in filtered_lines:\n            keywords = self.model.extract_keywords(\n                line,\n                keyphrase_ngram_range=self.keyphrase_ngram_range,\n                stop_words=self.stop_words,\n                top_n=1\n            )\n            if keywords:\n                titles.add(keywords[0][0])\n        return list(titles)\n\n    def extract_semantic_titles(self, top_n=10):\n        sentences = sent_tokenize(self.full_text)\n        joined_text = \" \".join(sentences)\n        keywords = self.model.extract_keywords(\n            joined_text,\n            keyphrase_ngram_range=self.keyphrase_ngram_range,\n            stop_words=self.stop_words,\n            use_maxsum=True,\n            top_n=top_n\n        )\n        return [kw[0] for kw in keywords]\n\n    def extract_titles_fontbased(self, pdf_path: str, top_n_fonts: int = 2) -> List[str]:\n\n        all_spans = []\n\n        # Single pass: extract all spans with size and content\n        with fitz.open(pdf_path) as pdf:\n            for page_num, page in enumerate(pdf):\n                blocks = page.get_text(\"dict\")[\"blocks\"]\n                for block in blocks:\n                    if \"lines\" in block:\n                        for line in block[\"lines\"]:\n                            for span in line[\"spans\"]:\n                                text = span[\"text\"].strip()\n                                if text:\n                                    all_spans.append({\n                                        \"text\": text,\n                                        \"size\": span[\"size\"],\n                                        \"page\": page_num + 1,\n                                        \"bbox\": span[\"bbox\"]\n                                    })\n\n        # Identify top N font sizes\n        font_sizes = np.array([span[\"size\"] for span in all_spans])\n        top_fonts = sorted(np.unique(font_sizes))[-top_n_fonts:]\n\n        # Filter and collect titles\n        titles = [\n            span[\"text\"] for span in all_spans\n            if span[\"size\"] in top_fonts\n        ]\n\n        # Optional: sort by page and vertical position\n        titles_with_position = [\n            (span[\"text\"], span[\"page\"], span[\"bbox\"][1])\n            for span in all_spans if span[\"size\"] in top_fonts\n        ]\n        titles_sorted = sorted(titles_with_position, key=lambda x: (x[1], x[2]))\n\n        # Return just the text\n        return [text for text, _, _ in titles_sorted]\n\n#----------------Query constructed--------------\nclass QueryConstructorLM:\n    def __init__(self, model_name=\"google/flan-t5-large\"):\n        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n\n    def reconstruct(self, titles: List[str], max_length=64) -> List[str]:\n        \"\"\"\n        Rewrites a list of section titles into detailed search queries.\n\n        Args:\n            titles: List of section title strings.\n            max_length: Max token length for generated queries.\n\n        Returns:\n            List of rewritten queries corresponding to each input title.\n        \"\"\"\n        # Prepare prompts for all titles\n        prompts = [\n            \"Convert section titles into research questions:\\n\"\n            \"Title: Introduction → What is the background and motivation of this study?\\n\"\n            \"Title: Evaluation → How was the model's performance evaluated?\\n\"\n            f\"Title: {title} →\"\n            for title in titles\n        ]\n\n        # Tokenize all prompts with padding for batch processing\n        inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n\n        # Generate outputs in batch\n        output_ids = self.model.generate(\n            input_ids=inputs.input_ids,\n            attention_mask=inputs.attention_mask,\n            max_length=max_length,\n            num_beams=5,\n            early_stopping=True,\n            no_repeat_ngram_size=2\n        )\n\n        # Decode each generated output\n        rewritten_queries = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n        # print(f\"reconstructed_queries: {rewritten_queries}\\n\")\n        return rewritten_queries\n\n#-------------------------Chunking------------------\nclass AdaptiveChunker:\n    def __init__(self,\n                 base_chunk_size: int = 200,\n                 chunk_overlap: int = 50):\n        self.splitter = RecursiveCharacterTextSplitter(\n            chunk_size=base_chunk_size,\n            chunk_overlap=chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n        )\n\n    def chunk_text(self, texts: List[Dict]) -> List[Dict]:\n        \"\"\"Split text into chunks and assign minimal, non-duplicate metadata.\"\"\"\n        chunks = []\n        for ti, text_data in enumerate(texts):\n            parts = self.splitter.split_text(text_data['content'])\n            for ci, part in enumerate(parts):\n                metadata = {\n                    'page': text_data.get('page', text_data.get('paragraph')),\n                    'chunk_index': ci,\n                    'total_chunks': len(parts),\n                    'chunk_type': \"text\"\n                }\n                chunk = {\n                    'text': part.strip(),\n                    'metadata': metadata\n                }\n\n                chunks.append(chunk)\n        return chunks\n\n# -------------------- CLIP Text Embedding --------------------\nclass CLIPTextEmbedder:\n    def __init__(self, model_id=IMAGE_MODEL_ID, device=DEVICE):\n        self.device = device\n        self.model = CLIPModel.from_pretrained(model_id).to(device)\n        self.processor = CLIPProcessor.from_pretrained(model_id)\n\n    def encode(self, texts, batch_size=32):\n        \"\"\"Encode text using CLIP text encoder\"\"\"\n        embeddings = []\n\n        # Process in batches to avoid memory issues\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i + batch_size]\n            inputs = self.processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n\n            with torch.no_grad():\n                text_features = self.model.get_text_features(**inputs)\n                text_features /= text_features.norm(p=2, dim=-1, keepdim=True)\n                embeddings.append(text_features.cpu().numpy())\n\n        return np.vstack(embeddings)\n\n# -------------------- Image Embedding --------------------\nclass ImageEmbedder:\n    def __init__(self, model_id=IMAGE_MODEL_ID, device=DEVICE):\n        self.device = device\n        self.model = CLIPModel.from_pretrained(model_id).to(device)\n        self.processor = CLIPProcessor.from_pretrained(model_id)\n\n    def encode(self, images):\n        embeddings = []\n        for img in images:\n            inputs = self.processor(images=img, return_tensors=\"pt\").to(self.device)\n            with torch.no_grad():\n                features = self.model.get_image_features(**inputs)\n                features /= features.norm(p=2, dim=-1, keepdim=True)\n                embeddings.append(features.cpu().numpy())\n        return np.vstack(embeddings)  # Stack the list of arrays into a single array\n\n#---------------Vector DB-------------------------------\nclass ChromaDBManager:\n    def __init__(self, persist_dir: str = \"./chroma_db\"):\n        self.client = chromadb.PersistentClient(path=persist_dir)\n\n        # Only need collections for content we'll retrieve\n        self.text_collection = self.client.get_or_create_collection(\n            name=\"document_texts\",\n            embedding_function=None  # We'll provide pre-computed embeddings\n        )\n        self.image_collection = self.client.get_or_create_collection(\n            name=\"document_images\",\n            embedding_function=None\n        )\n\n    def store_texts(self, texts: List[str], embeddings: np.ndarray) -> List[str]:\n        \"\"\"Store text embeddings\"\"\"\n        ids = [str(uuid.uuid4()) for _ in texts]\n        self.text_collection.add(\n            ids=ids,\n            documents=texts,\n            embeddings=embeddings.tolist()\n        )\n        return ids\n\n    def store_images(self, image_indices: List[int], embeddings: np.ndarray) -> List[str]:\n        \"\"\"Store image embeddings\"\"\"\n        ids = [str(uuid.uuid4()) for _ in image_indices]\n        self.image_collection.add(\n            ids=ids,\n            # uris=image_indices,\n            embeddings=embeddings.tolist(),\n            metadatas=[{\"index\": idx} for idx in image_indices]\n        )\n        return ids\n\n\n    def query_texts(self, query_embedding: np.ndarray, n_results: int = 5) -> List[str]:\n        \"\"\"Query similar texts\"\"\"\n        results = self.text_collection.query(\n            query_embeddings=query_embedding.tolist(),\n            n_results=n_results\n        )\n        return results['documents'][0] if results['documents'] else []\n\n    def query_images(self, query_embedding: np.ndarray, n_results: int = 3) -> List[int]:\n        \"\"\"Query similar images and return their indices\"\"\"\n        results = self.image_collection.query(\n            query_embeddings=query_embedding.tolist(),\n            n_results=n_results\n        )\n        if not results['metadatas']:\n            return []\n\n        # Extract indices from metadata\n        return [meta['index'] for meta in results['metadatas'][0] if 'index' in meta]\n\n#--------- Summarizer Class----------------------------------\nclass Summarizer:\n    \"\"\"\n    Handles text summarization using multiple backends and prompt strategies.\n    Falls back to a local Hugging Face model if the primary API fails.\n    \"\"\"\n    def __init__(self, gemini_api_key: str = \"AIzaSyDyh7ZgcDa_FGekrfoNMnRt5UESG_iBgfI\", hf_model: str = \"facebook/bart-large-cnn\"):\n        self.hf_summarizer = pipeline(\"summarization\", model=hf_model)\n        self.gemini_model = None\n\n        if gemini_api_key:\n            try:\n                genai.configure(api_key=gemini_api_key)\n                self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-lite')\n                print(\"INFO: Google Gemini initialized successfully.\")\n            except Exception as e:\n                warnings.warn(f\"Failed to initialize Gemini, will use Hugging Face only. Error: {e}\")\n        else:\n            print(\"INFO: No Gemini API key provided. Using Hugging Face model as default.\")\n\n    def _get_zero_shot_prompt(self, text: str) -> str:\n        return f\"\"\"\n        You are an expert document analyst tasked with creating\n        a comprehensive summary of the entire document.\n        Summarize the following text into a list of concise, factual bullet points.\n        Each bullet point should capture a key piece of information.\n\n        TEXT:\n        \"{text}\"\n\n        GUIDLINES:\n        Directly output the bullet points without any introductory sentences like \"Here is a summary:\n\n        BULLET POINTS:\n        \"\"\"\n\n    def _get_few_shot_prompt(self, text: str) -> str:\n        return f\"\"\"\n        You are an expert document analyst tasked with creating\n        a comprehensive summary of the entire document.\n        Summarize the following text into a list of concise, factual bullet points.\n        Each bullet point should capture a key piece of information.\n\n        GUIDLINES:\n        Directly output the bullet points without any introductory sentences like \"Here is a summary:\n\n        ---\n        Example 1:\n        TEXT: \"The sun is a star at the center of the Solar System. It is a nearly perfect ball of hot plasma, heated to incandescence by nuclear fusion reactions in its core.\"\n        BULLET POINTS:\n        - The sun is the star at the center of our Solar System.\n        - It is a ball of hot plasma.\n        - Nuclear fusion in its core generates heat.\n        ---\n        Example 2:\n        TEXT: \"Mars is the fourth planet from the Sun and the second-smallest planet in the Solar System, being larger than only Mercury. In English, Mars carries the name of the Roman god of war and is often referred to as the 'Red Planet'.\"\n        BULLET POINTS:\n        - Mars is the fourth planet from the Sun.\n        - It is the second-smallest planet in the Solar System.\n        - It is named after the Roman god of war.\n        - It is commonly called the \"Red Planet\".\n        ---\n        Your Turn:\n        TEXT: \"{text}\"\n        BULLET POINTS:\n        \"\"\"\n\n    def _get_cot_prompt(self, text: str) -> str:\n        return f\"\"\"\n        You are an expert document analyst tasked with creating\n        a comprehensive summary of the entire document.\n        Summarize the following text into a list of concise, factual bullet points.\n        Each bullet point should capture a key piece of information.\n\n        GUIDLINES:\n        Directly output the bullet points without any introductory sentences like \"Here is a summary:\n\n\n        Summarize the text below by following these steps:\n        1. First, identify the 3-5 most important concepts or key takeaways from the text.\n        2. Second, based on those key takeaways, generate a list of concise bullet points that accurately represent the text.\n\n        TEXT:\n        \"{text}\"\n\n        RESPONSE:\n        \"\"\"\n\n    def _to_bullets(self, summary_text: str) -> list[str]:\n        \"\"\"Cleans and splits a block of text into a list of bullet points.\"\"\"\n        # Remove markdown-style bullets and split by newlines or punctuation\n        cleaned_text = re.sub(r'^\\s*[-*]\\s*', '', summary_text, flags=re.MULTILINE)\n        bullets = [b.strip() for b in re.split(r'\\n|[.;]\\s*', cleaned_text) if b.strip()]\n        return bullets if bullets else [summary_text]\n\n    def summarize(self, text: str, strategy: str = 'zero-shot') -> list[str]:\n        \"\"\"\n        Summarizes text into bullet points. Tries Gemini first, then falls back to HF.\n\n        Args:\n            text (str): The text to summarize.\n            strategy (str): The prompt strategy to use ('zero-shot', 'few-shot', 'cot').\n                            This only applies to the Gemini backend.\n\n        Returns:\n            list[str]: A list of bullet points.\n        \"\"\"\n        if not text or len(text.strip()) < 50:\n            return [text.strip()] if text.strip() else []\n\n        # --- Attempt 1: Use Gemini with specified strategy ---\n        if self.gemini_model:\n            try:\n                print(f\"INFO: Attempting summarization with Gemini (strategy: {strategy})...\")\n                prompt_map = {\n                    'zero-shot': self._get_zero_shot_prompt,\n                    'few-shot': self._get_few_shot_prompt,\n                    'cot': self._get_cot_prompt,\n                }\n                prompt_func = prompt_map.get(strategy, self._get_zero_shot_prompt)\n                prompt = prompt_func(text)\n\n                response = self.gemini_model.generate_content(prompt)\n                summary = response.text\n                return self._to_bullets(summary)\n\n            except Exception as e:\n                warnings.warn(f\"Gemini summarization failed: {e}. Falling back to Hugging Face model.\")\n\n        # --- Attempt 2: Fallback to Hugging Face ---\n        print(\"INFO: Using fallback Hugging Face model for summarization...\")\n        try:\n            # Truncate text for models with limited input size\n            max_input_length = 1024\n            truncated_text = text[:max_input_length] if len(text) > max_input_length else text\n\n            summary_result = self.hf_summarizer(truncated_text, max_length=150, min_length=40, do_sample=False)\n            summary = summary_result[0]['summary_text']\n            return self._to_bullets(summary)\n\n        except Exception as e:\n            warnings.warn(f\"Hugging Face summarization also failed: {e}\")\n            # Final fallback: just split the original text into sentences\n            from nltk.tokenize import sent_tokenize\n            return sent_tokenize(text)[:5]\n\n#----------------SlideBuilder-----------------------------\nclass SlideBuilder:\n    def __init__(self, titles, title_embeddings, lines, images: list, # Type hint clarifies it's a list\n                 db_manager: ChromaDBManager,\n                 summarizer: Summarizer,\n                 max_bullets_per_slide=5, overlap_bullets=1):\n\n        self.titles = titles\n        self.title_embeddings = title_embeddings\n        self.lines = lines\n        self.images = images # This is now a list\n        self.db_manager = db_manager\n        self.summarizer = summarizer\n        self.max_bullets = max_bullets_per_slide\n        self.overlap_bullets = overlap_bullets\n\n        # REMOVED: self.image_indices = list(images.keys())\n        # This line was incorrect for a list and has been removed.\n\n    def build_slides(self, summarization_strategy: str = 'zero-shot', max_image_occurrences: int = 1):\n        \"\"\"Builds slides with controlled image deduplication.\"\"\"\n        slides = []\n        previous_bullets = []\n        image_usage_counter = collections.Counter()\n\n        for title, title_emb in zip(self.titles, self.title_embeddings):\n            relevant_texts = self.db_manager.query_texts(title_emb, n_results=20)\n            if not relevant_texts:\n                slides.append({'title': title, 'bullets': [f\"No content for: {title}\"], 'images': []})\n                continue\n\n            combined_text = \" \".join(relevant_texts)\n            bullets = self.summarizer.summarize(combined_text, strategy=summarization_strategy)\n\n            start_idx = 0\n            while start_idx < len(bullets):\n                overlap = previous_bullets[-self.overlap_bullets:] if start_idx > 0 and previous_bullets else []\n                end_idx = start_idx + self.max_bullets - len(overlap)\n                slide_bullets = overlap + bullets[start_idx:end_idx]\n\n                selected_image_indices = self._retrieve_images(\n                    query_embedding=title_emb,\n                    image_usage_counter=image_usage_counter,\n                    max_occurrences=max_image_occurrences\n                )\n\n                for img_idx in selected_image_indices:\n                    image_usage_counter[img_idx] += 1\n\n                # This line works correctly for a list, e.g., self.images[0]\n                slide_images = [self.images[idx] for idx in selected_image_indices]\n\n                slides.append({\n                    'title': title,\n                    'bullets': slide_bullets,\n                    'images': slide_images\n                })\n\n                previous_bullets = slide_bullets\n                start_idx = end_idx\n                if not bullets[start_idx:end_idx]:\n                    break\n        return slides\n\n    def _retrieve_images(self, query_embedding, image_usage_counter, max_occurrences, n_results=3, candidate_pool_size=20):\n        \"\"\"Retrieves relevant and non-overused images from a list.\"\"\"\n        selected_indices = []\n        candidate_indices = self.db_manager.query_images(query_embedding, n_results=candidate_pool_size)\n\n        for idx in candidate_indices:\n            if len(selected_indices) >= n_results:\n                break\n\n            # The logic relies on the corrected _image_exists method\n            if self._image_exists(idx) and image_usage_counter[idx] < max_occurrences:\n                if idx not in selected_indices:\n                    selected_indices.append(idx)\n\n        return selected_indices\n\n    # --- THIS IS THE CRITICAL CHANGE ---\n    def _image_exists(self, image_index):\n        \"\"\"Checks if an image index is valid for the images list.\"\"\"\n        # An index is valid if it's an integer and falls within the list's bounds.\n        return isinstance(image_index, int) and 0 <= image_index < len(self.images)\n\n# -------------------- Pipeline Orchestration --------------------\nclass DocumentProcessingPipeline:\n    def __init__(\n        self,\n        file_path,\n        title_option: Literal['fontbased', 'semantic', 'candidate'] = 'fontbased',\n        persist_db: bool = True,\n        top_titles: int = 20,\n        extractor: Optional[DocumentExtractor] = None,\n        extractor_config: dict = None,\n        query_constructor: Optional[QueryConstructorLM] = None,\n        query_constructor_config: dict = None,\n        text_embedder: Optional[CLIPTextEmbedder] = None,\n        text_embedder_config: dict = None,\n        image_embedder: Optional[ImageEmbedder] = None,\n        image_embedder_config: dict = None,\n        chunker: Optional[AdaptiveChunker] = None,\n        chunker_config: dict = None,\n        db_manager: Optional[ChromaDBManager] = None,\n        db_manager_config: dict = None,\n        summarizer: Optional[Summarizer] = None,\n        summarizer_config: dict = None,\n    ):\n        self.file_path = file_path\n        self.title_option = title_option\n        self.top_titles = top_titles\n\n        self.extractor = extractor or DocumentExtractor(**(extractor_config or {}))\n        self.query_constructor = query_constructor or QueryConstructorLM(**(query_constructor_config or {}))\n        self.text_embedder = text_embedder or CLIPTextEmbedder(**(text_embedder_config or {}))\n        self.image_embedder = image_embedder or ImageEmbedder(**(image_embedder_config or {}))\n        self.chunker = chunker or AdaptiveChunker(**(chunker_config or {}))\n        self.db_manager = db_manager or (ChromaDBManager(**(db_manager_config or {})) if persist_db else None)\n        self.summarizer = summarizer or Summarizer(**(summarizer_config or {}))\n\n    def run(self):\n          print(f\"Extracting content from {self.file_path}\")\n\n          raw_text, images, tables = self.extractor.extract(self.file_path)\n          raw_text_cp = raw_text.copy()\n          raw_text =  [item[\"content\"] for item in raw_text]\n          raw_text = \"\".join(raw_text)\n\n          images =  [item[\"image\"] for item in images]\n\n\n\n          # lines = [line.strip() for line in raw_text.split('\\n') if line.strip()]       # for chunking of lines\n          chunks = self.chunker.chunk_text(raw_text_cp)\n          lines = [item['text'] for item in chunks]                                     # for Recursive(Adaptive) chunks\n\n\n\n          self.title_extractor = TitleExtractor(lines=lines, full_text=raw_text)\n          titles = self.title_extractor.extract_titles(\n              option=self.title_option,\n              pdf_path=self.file_path,\n              top_n=self.top_titles\n          )\n\n\n\n          # constructed_queries = self.query_constructor.reconstruct(\n          #     [title for title in titles]\n          # )\n\n\n\n\n          # Embed all content\n          text_embeddings = self.text_embedder.encode(lines)\n          image_embeddings = self.image_embedder.encode(images) if images else []\n          title_embeddings = self.text_embedder.encode(titles)\n\n          # Store in ChromaDB if enabled\n          if self.db_manager:\n              self.db_manager.store_texts(lines, text_embeddings)\n              if images:\n                  image_indices = list(range(len(images)))\n                  self.db_manager.store_images(image_indices, image_embeddings)\n\n          return {\n              \"titles\": [title.capitalize() for title in titles],\n              # \"constructed_queries\": constructed_queries,\n              \"title_embeddings\": title_embeddings,\n              \"lines\": lines,\n              \"images\": images,\n              \"db_manager\": self.db_manager,\n              \"text_embeddings\": text_embeddings,\n              \"image_embeddings\": image_embeddings,\n              \"text_embedder\": self.text_embedder,\n              \"image_embedder\": self.image_embedder,\n              \"title_extractor\": self.title_extractor,\n              \"title_option\": self.title_option,\n              \"raw_text\": raw_text,\n              \"tables\": tables,\n              \"chunks\": chunks,\n              \"summarizer\": self.summarizer\n\n          }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:46:46.823896Z","iopub.execute_input":"2025-06-12T22:46:46.824123Z","iopub.status.idle":"2025-06-12T22:46:46.882106Z","shell.execute_reply.started":"2025-06-12T22:46:46.824105Z","shell.execute_reply":"2025-06-12T22:46:46.881431Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:46:46.884000Z","iopub.execute_input":"2025-06-12T22:46:46.884272Z","iopub.status.idle":"2025-06-12T22:46:46.924683Z","shell.execute_reply.started":"2025-06-12T22:46:46.884254Z","shell.execute_reply":"2025-06-12T22:46:46.923852Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Presentation Builder","metadata":{}},{"cell_type":"code","source":"# very good deep\nimport os\nfrom pptx import Presentation\nfrom pptx.util import Inches, Pt\nfrom pptx.dml.color import RGBColor\nfrom PIL import Image\nimport textwrap\n\nclass PresentationBuilder:\n    def __init__(self, title=\"Presentation Title\", template_path=None, font_name=\"Arial\", font_size=20):\n        self.title = title\n        self.presentation = Presentation(template_path) if template_path else Presentation()\n\n        # Set standard 16:9 size (13.333\" x 7.5\") if no template\n        if not template_path:\n            self.presentation.slide_width = Inches(13.333)\n            self.presentation.slide_height = Inches(7.5)\n\n        self.font_name = font_name\n        self.font_size = Pt(font_size)\n        self.title_font_size = Pt(font_size + 10)\n        self.slide_width = self.presentation.slide_width\n        self.slide_height = self.presentation.slide_height\n\n        # Margins for 16:9 slides\n        self.left_margin = Inches(1.0)\n        self.right_margin = Inches(1.0)\n        self.top_margin = Inches(0.5)\n        self.bottom_margin = Inches(0.5)\n\n        # Increased title area for image-only slides\n        self.title_height = Inches(1.5)  # Increased from 1.0 to prevent overlap\n\n        self._add_title_slide()\n\n    def _add_title_slide(self):\n        slide_layout = self.presentation.slide_layouts[0]\n        slide = self.presentation.slides.add_slide(slide_layout)\n        title = slide.shapes.title\n        title.text = self.title\n        title.text_frame.paragraphs[0].font.size = Pt(44)\n        title.text_frame.paragraphs[0].font.name = self.font_name\n        title.text_frame.paragraphs[0].font.bold = True\n\n    def _calculate_text_height(self, bullets):\n        if not bullets:\n            return Inches(0)\n\n        avg_line_height = self.font_size.pt * 1.2 / 72  # in inches\n        total_lines = sum(max(1, len(textwrap.wrap(bullet, width=70))) for bullet in bullets)\n        return Inches(total_lines * avg_line_height + len(bullets) * 0.1)\n\n    def _add_bullet_points(self, slide, bullets):\n        left = self.left_margin\n        top = self.top_margin + self.title_height\n        width = self.slide_width - self.left_margin - self.right_margin\n        height = self._calculate_text_height(bullets)\n\n        textbox = slide.shapes.add_textbox(left, top, width, height)\n        tf = textbox.text_frame\n        tf.word_wrap = True\n\n        for idx, bullet in enumerate(bullets):\n            p = tf.add_paragraph() if idx > 0 else tf.paragraphs[0]\n            p.text = bullet\n            p.level = 0\n            p.font.name = self.font_name\n            p.font.size = self.font_size\n            p.space_after = Pt(12)\n\n        return top + height\n\n    def _calculate_image_size(self, img, max_width, max_height):\n        img_width, img_height = img.size\n        img_ratio = img_width / img_height\n\n        max_width_px = max_width * 96\n        max_height_px = max_height * 96\n\n        if (max_width_px / img_ratio) <= max_height_px:\n            width = max_width_px\n            height = width / img_ratio\n        else:\n            height = max_height_px\n            width = height * img_ratio\n\n        return Inches(width / 96), Inches(height / 96)\n\n    def _add_images_to_slide(self, slide, images, content_bottom, slide_title=\"\"):\n        available_width = (self.slide_width - self.left_margin - self.right_margin) / Inches(1)\n        available_height = (self.slide_height - content_bottom - self.bottom_margin) / Inches(1)\n\n        images_added = 0\n        current_top = content_bottom + Inches(0.2)\n\n        for idx, img in enumerate(images):\n            width, height = self._calculate_image_size(img, available_width, available_height)\n\n            if current_top + height > self.slide_height - self.bottom_margin:\n                self._create_image_slides(images[idx:], slide_title)\n                break\n\n            left_pos = (self.slide_width - width) / 2\n\n            temp_path = f\"temp_img_{idx}.png\"\n            try:\n                img.save(temp_path)\n                slide.shapes.add_picture(\n                    temp_path,\n                    left_pos,\n                    current_top,\n                    width,\n                    height\n                )\n                images_added += 1\n                current_top += height + Inches(0.2)\n            except Exception as e:\n                print(f\"Error adding image: {e}\")\n            finally:\n                if os.path.exists(temp_path):\n                    os.remove(temp_path)\n\n        return images_added\n\n    def _create_image_slides(self, images, base_title):\n        \"\"\"Create dedicated slides for images with expanded title area\"\"\"\n        # Calculate available space for image (with increased title area)\n        content_top = self.top_margin + self.title_height\n        max_width = (self.slide_width - self.left_margin - self.right_margin) / Inches(1)\n        max_height = (self.slide_height - content_top - self.bottom_margin) / Inches(1)\n\n        for idx, img in enumerate(images):\n            slide = self.presentation.slides.add_slide(self.presentation.slide_layouts[5])\n\n            # Add title with expanded space\n            title = f\"{base_title} - Image {idx+1}\" if base_title else f\"Image {idx+1}\"\n            self._add_expanded_slide_title(slide, title)\n\n            # Calculate image size\n            width, height = self._calculate_image_size(img, max_width, max_height)\n\n            # Center image in remaining space\n            left = (self.slide_width - width) / 2\n            top = content_top + (self.slide_height - content_top - height - self.bottom_margin) / 2\n\n            # Add image\n            temp_path = f\"temp_full_img_{idx}.png\"\n            try:\n                img.save(temp_path)\n                slide.shapes.add_picture(\n                    temp_path,\n                    left,\n                    top,\n                    width,\n                    height\n                )\n            except Exception as e:\n                print(f\"Error adding full image: {e}\")\n            finally:\n                if os.path.exists(temp_path):\n                    os.remove(temp_path)\n\n    def _add_expanded_slide_title(self, slide, title):\n        \"\"\"Add title with more vertical space on image-only slides\"\"\"\n        title_shape = slide.shapes.add_textbox(\n            left=self.left_margin,\n            top=self.top_margin,\n            width=self.slide_width - self.left_margin - self.right_margin,\n            height=self.title_height  # Using the increased height\n        )\n        tf = title_shape.text_frame\n        p = tf.paragraphs[0]\n        p.text = title\n        p.font.name = self.font_name\n        p.font.size = self.title_font_size\n        p.font.bold = True\n        p.alignment = 1  # Center aligned\n\n    def _add_slide_title(self, slide, title):\n        if hasattr(slide.shapes, 'title'):\n            title_shape = slide.shapes.title\n            title_shape.text = title\n            tf = title_shape.text_frame\n            p = tf.paragraphs[0]\n            p.font.name = self.font_name\n            p.font.size = self.title_font_size\n            p.font.bold = True\n        else:\n            self._add_expanded_slide_title(slide, title)\n\n    def add_slide(self, slide_data):\n        title = slide_data.get(\"title\", \"\")\n        bullets = slide_data.get(\"bullets\", [])\n        images = slide_data.get(\"images\", [])\n\n        pil_images = []\n        for img in images:\n            if isinstance(img, str):\n                try:\n                    pil_images.append(Image.open(img))\n                except Exception as e:\n                    print(f\"Error loading image {img}: {e}\")\n            elif isinstance(img, Image.Image):\n                pil_images.append(img)\n\n        layout = 1 if bullets else 5\n        slide = self.presentation.slides.add_slide(self.presentation.slide_layouts[layout])\n        self._add_slide_title(slide, title)\n\n        content_bottom = self.top_margin + self.title_height\n        if bullets:\n            content_bottom = self._add_bullet_points(slide, bullets)\n\n        if pil_images:\n            added = self._add_images_to_slide(slide, pil_images, content_bottom, title)\n            # print(f\"Added {added} image(s) to slide\")\n\n    def build_from_list(self, slides):\n        for slide_data in slides:\n            self.add_slide(slide_data)\n\n    def save(self, filename=\"output_presentation.pptx\"):\n        self.presentation.save(filename)\n        print(f\"Presentation saved to {filename}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-06-12T22:46:46.925586Z","iopub.execute_input":"2025-06-12T22:46:46.925816Z","iopub.status.idle":"2025-06-12T22:46:47.000731Z","shell.execute_reply.started":"2025-06-12T22:46:46.925793Z","shell.execute_reply":"2025-06-12T22:46:46.999938Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Example","metadata":{}},{"cell_type":"markdown","source":"#### Extract titles Phase","metadata":{}},{"cell_type":"code","source":"# # Usage example - now both will be 512-dimensional\n# file_path = \"/kaggle/input/sample1/2107.07382v1.pdf\" # or \"your_document.docx\"\n\n\n# doc_pipeline = DocumentProcessingPipeline(file_path, \"semantic\")\n# results = doc_pipeline.run()\n\n# # Both text_embeddings and title_embeddings will now be 512-dimensional from CLIP\n# title_embeddings = doc_pipeline.text_embedder.encode(results[\"titles\"])\n# print(f\"Embedded {len(results['titles'])} titles.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:46:47.001473Z","iopub.execute_input":"2025-06-12T22:46:47.001673Z","iopub.status.idle":"2025-06-12T22:46:47.005457Z","shell.execute_reply.started":"2025-06-12T22:46:47.001657Z","shell.execute_reply":"2025-06-12T22:46:47.004610Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"#### Slides JSON Format for Selected Titles","metadata":{}},{"cell_type":"code","source":"# slide_builder = SlideBuilder(\n#     titles=[title.capitalize() for title in results[\"titles\"]],\n#     # titles=results[\"constructed_queries\"],\n#     lines=results[\"lines\"],\n#     images=results[\"images\"],\n#     db_manager=results[\"db_manager\"],\n#     # text_embedder = results[\"text_embedder\"],\n#     title_embeddings = results[\"title_embeddings\"],\n#     summarizer = results[\"summarizer\"],\n#     # title_embeddings = results[\"text_embedder\"].encode(results[\"constructed_queries\"]),\n# )\n\n# slides = slide_builder.build_slides(summarization_strategy=\"cot\", max_image_occurrences=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:46:47.006107Z","iopub.execute_input":"2025-06-12T22:46:47.006359Z","iopub.status.idle":"2025-06-12T22:46:47.019799Z","shell.execute_reply.started":"2025-06-12T22:46:47.006342Z","shell.execute_reply":"2025-06-12T22:46:47.019057Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"#### Build the pptx file using the slides generated","metadata":{}},{"cell_type":"code","source":"# builder = PresentationBuilder(\n#         title=\"LLM survey\",\n#         font_name=\"Arial\",\n#         font_size=20,\n#         # template_path=\"/kaggle/working/template_2.pptx\"\n#     )\n\n# builder.build_from_list(slides)\n# builder.save(\"/kaggle/working/LLM_survey.pptx\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:46:47.020689Z","iopub.execute_input":"2025-06-12T22:46:47.021475Z","iopub.status.idle":"2025-06-12T22:46:47.030698Z","shell.execute_reply.started":"2025-06-12T22:46:47.021443Z","shell.execute_reply":"2025-06-12T22:46:47.029940Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# GA","metadata":{}},{"cell_type":"code","source":"import random\nimport copy\nfrom typing import List, Dict, Any, Tuple\nfrom collections import defaultdict\n\nimport tempfile\nfrom pptx import Presentation\nfrom pptx.util import Inches, Pt\nfrom pptx.enum.text import PP_ALIGN, MSO_ANCHOR\nfrom pptx.enum.shapes import MSO_SHAPE_TYPE\nfrom pptx.dml.color import RGBColor\nfrom pptx.enum.dml import MSO_THEME_COLOR\n\nclass SlideLayout:\n    \"\"\"Represents a single slide layout (chromosome)\"\"\"\n\n    def __init__(self, genes: Dict[str, Any] = None):\n        if genes is None:\n            self.genes = self._generate_random_genes()\n        else:\n            self.genes = genes\n        self.fitness_score = 0\n\n    def _generate_random_genes(self) -> Dict[str, Any]:\n        \"\"\"Generate random layout genes\"\"\"\n        return {\n            \"title_position\": random.choice([\"top\", \"center\", \"left\", \"right\"]),\n            \"bullet_columns\": random.choice([1, 2]),\n            \"bullet_font_size\": random.randint(16, 36),\n            \"image_position\": random.choice([\"left\", \"right\", \"top\", \"bottom\", \"grid\", \"none\"]),\n            \"image_size\": random.choice([\"small\", \"medium\", \"large\"]),\n            \"image_layout\": random.choice([\"single\", \"grid\", \"horizontal\", \"vertical\"]),\n            \"theme\": random.choice([\"default\", \"dark\", \"light\", \"modern\"]),\n            \"margin_size\": random.choice([\"small\", \"medium\", \"large\"]),\n            \"title_font_size\": random.randint(24, 48),\n            \"background_color\": random.choice([\"white\", \"light_gray\", \"dark_blue\", \"light_blue\"]),\n            \"text_color\": random.choice([\"black\", \"white\", \"dark_gray\", \"blue\"]),\n            \"image_text_balance\": random.choice([\"text_heavy\", \"balanced\", \"image_heavy\"])\n        }\n\n    def mutate(self, mutation_rate: float = 0.1):\n        \"\"\"Mutate the layout genes\"\"\"\n        gene_options = {\n            \"title_position\": [\"top\", \"center\", \"left\", \"right\"],\n            \"bullet_columns\": [1, 2],\n            \"bullet_font_size\": list(range(16, 37)),\n            \"image_position\": [\"left\", \"right\", \"top\", \"bottom\", \"grid\", \"none\"],\n            \"image_size\": [\"small\", \"medium\", \"large\"],\n            \"image_layout\": [\"single\", \"grid\", \"horizontal\", \"vertical\"],\n            \"theme\": [\"default\", \"dark\", \"light\", \"modern\"],\n            \"margin_size\": [\"small\", \"medium\", \"large\"],\n            \"title_font_size\": list(range(24, 49)),\n            \"background_color\": [\"white\", \"light_gray\", \"dark_blue\", \"light_blue\"],\n            \"text_color\": [\"black\", \"white\", \"dark_gray\", \"blue\"],\n            \"image_text_balance\": [\"text_heavy\", \"balanced\", \"image_heavy\"]\n        }\n\n        for gene_name in self.genes:\n            if random.random() < mutation_rate:\n                self.genes[gene_name] = random.choice(gene_options[gene_name])\n\n    def crossover(self, other: 'SlideLayout') -> 'SlideLayout':\n        \"\"\"Create offspring through crossover\"\"\"\n        child_genes = {}\n        for gene_name in self.genes:\n            child_genes[gene_name] = random.choice([self.genes[gene_name], other.genes[gene_name]])\n        return SlideLayout(child_genes)\n\n    def __str__(self):\n        return f\"Layout(fitness={self.fitness_score:.2f}, genes={self.genes})\"\n\n\nclass ImageAnalyzer:\n    \"\"\"Utility class for analyzing PIL Image objects\"\"\"\n\n    @staticmethod\n    def get_image_properties(image: Image.Image) -> Dict[str, Any]:\n        \"\"\"Extract properties from PIL Image\"\"\"\n        width, height = image.size\n        aspect_ratio = width / height\n\n        return {\n            'width': width,\n            'height': height,\n            'aspect_ratio': aspect_ratio,\n            'orientation': 'landscape' if aspect_ratio > 1.2 else 'portrait' if aspect_ratio < 0.8 else 'square',\n            'total_pixels': width * height,\n            'is_large': (width * height) > 500000,  # > 0.5MP\n            'is_wide': aspect_ratio > 1.5,\n            'is_tall': aspect_ratio < 0.7\n        }\n\n    @staticmethod\n    def analyze_image_collection(images: List[Image.Image]) -> Dict[str, Any]:\n        \"\"\"Analyze a collection of images\"\"\"\n        if not images:\n            return {\n                'count': 0,\n                'has_images': False,\n                'mixed_orientations': False,\n                'avg_aspect_ratio': 1.0,\n                'total_area': 0,\n                'complexity_score': 0\n            }\n\n        properties = [ImageAnalyzer.get_image_properties(img) for img in images]\n        orientations = [prop['orientation'] for prop in properties]\n        aspect_ratios = [prop['aspect_ratio'] for prop in properties]\n        total_pixels = sum(prop['total_pixels'] for prop in properties)\n\n        return {\n            'count': len(images),\n            'has_images': True,\n            'mixed_orientations': len(set(orientations)) > 1,\n            'avg_aspect_ratio': np.mean(aspect_ratios),\n            'dominant_orientation': max(set(orientations), key=orientations.count),\n            'total_area': total_pixels,\n            'has_large_images': any(prop['is_large'] for prop in properties),\n            'complexity_score': len(images) + (0.5 if len(set(orientations)) > 1 else 0)\n        }\n\n\nclass SlideContent:\n    \"\"\"Represents slide content with PIL Images\"\"\"\n\n    def __init__(self, title: str, bullets: List[str], images: List[Image.Image]):\n        self.title = title\n        self.bullets = bullets\n        self.images = images\n        self.word_count = len(title.split()) + sum(len(bullet.split()) for bullet in bullets)\n\n        # Analyze images\n        self.image_analysis = ImageAnalyzer.analyze_image_collection(images)\n\n        # Content density metrics\n        self.content_density = self._calculate_content_density()\n        self.text_image_ratio = self._calculate_text_image_ratio()\n\n    def _calculate_content_density(self) -> float:\n        \"\"\"Calculate overall content density\"\"\"\n        text_density = self.word_count / 100.0  # Normalize\n        image_density = self.image_analysis['complexity_score']\n        return text_density + image_density\n\n    def _calculate_text_image_ratio(self) -> str:\n        \"\"\"Determine if content is text-heavy, image-heavy, or balanced\"\"\"\n        if not self.images:\n            return \"text_only\"\n        elif len(self.images) >= 3 or self.image_analysis['total_area'] > 1000000:\n            return \"image_heavy\"\n        elif self.word_count > 100:\n            return \"text_heavy\"\n        else:\n            return \"balanced\"\n\n\nclass FitnessEvaluator:\n    \"\"\"Handles fitness evaluation for slide layouts\"\"\"\n\n    def __init__(self):\n        self.color_contrast_map = {\n            (\"black\", \"white\"): 1.0,\n            (\"black\", \"light_gray\"): 0.8,\n            (\"white\", \"dark_blue\"): 0.9,\n            (\"white\", \"light_blue\"): 0.6,\n            (\"dark_gray\", \"white\"): 0.9,\n            (\"blue\", \"white\"): 0.8,\n            (\"black\", \"dark_blue\"): 0.3,  # Poor contrast\n            (\"white\", \"light_gray\"): 0.4   # Poor contrast\n        }\n\n        # Slide dimensions (standard 16:9 aspect ratio) in inches\n        self.slide_width = 10.0  # inches\n        self.slide_height = 7.5  # inches\n\n        # Margin size mappings (in inches)\n        self.margin_sizes = {\n            'small': 0.3,\n            'medium': 0.5,\n            'large': 0.8\n        }\n\n        # Font size to height conversion (approximate)\n        self.font_height_ratio = 1.2  # font height = font_size * ratio\n        self.points_to_inches = 1/72  # 72 points = 1 inch\n\n        # Position mappings for layout calculation\n        self.title_positions = {\n            'top': {'x': 0.5, 'y': 0.1},  # centered at top\n            'center': {'x': 0.5, 'y': 0.5},  # centered\n            'left': {'x': 0.2, 'y': 0.2},  # left side\n            'right': {'x': 0.8, 'y': 0.2}  # right side\n        }\n\n        # Image position boundaries\n        self.image_positions = {\n            'left': {'x': 0.0, 'y': 0.3, 'max_width': 0.4},\n            'right': {'x': 0.6, 'y': 0.3, 'max_width': 0.4},\n            'top': {'x': 0.1, 'y': 0.0, 'max_height': 0.4},\n            'bottom': {'x': 0.1, 'y': 0.6, 'max_height': 0.4},\n            'grid': {'x': 0.1, 'y': 0.3, 'max_width': 0.8, 'max_height': 0.6}\n        }\n\n        # Size mappings for images (as fraction of slide)\n        self.image_size_ratios = {\n            'small': 0.2,\n            'medium': 0.35,\n            'large': 0.5\n        }\n\n    def evaluate_fitness(self, layout: SlideLayout, content: SlideContent,\n                        previous_layout: SlideLayout = None) -> float:\n        \"\"\"Comprehensive fitness evaluation\"\"\"\n        score = 100  # Start with base score\n\n        # 1. Text Readability\n        score += self._evaluate_text_readability(layout, content)\n\n        # 2. Bullet List Management\n        score += self._evaluate_bullet_management(layout, content)\n\n        # 3. Image Handling\n        score += self._evaluate_image_handling_advanced(layout, content)\n\n        # 4. Visual Balance\n        score += self._evaluate_visual_balance_advanced(layout, content)\n\n        # 5. Color Contrast\n        score += self._evaluate_color_contrast(layout)\n\n        # 6. Consistency\n        score += self._evaluate_consistency(layout, previous_layout)\n\n        # 7. Content Appropriateness\n        score += self._evaluate_content_appropriateness(layout, content)\n\n        # 8. Accessibility\n        score += self._evaluate_accessibility(layout)\n\n        # 9. Image-Text Balance\n        score += self._evaluate_image_text_balance(layout, content)\n\n        # 10. Boundary Constraints\n        score += self._evaluate_boundary_constraints(layout, content)\n\n        return max(0, score)  # Ensure non-negative score\n\n    def _evaluate_text_readability(self, layout: SlideLayout, content: SlideContent) -> float:\n        \"\"\"Evaluate text readability factors\"\"\"\n        score = 0\n\n        # Bullet font size\n        bullet_size = layout.genes['bullet_font_size']\n        if 16 <= bullet_size <= 28:\n            score += 10\n        elif 13 <= bullet_size <= 32:\n            score += 5\n        else:\n            score -= 15\n\n        # Title font size\n        title_size = layout.genes['title_font_size']\n        if 30 <= title_size <= 40:\n            score += 8\n        elif 24 <= title_size <= 44:\n            score += 4\n        else:\n            score -= 10\n\n        # Font size relationship (title should be larger than bullets)\n        if title_size > bullet_size:\n            score += 5\n        else:\n            score -= 8\n\n        return score\n\n    def _evaluate_bullet_management(self, layout: SlideLayout, content: SlideContent) -> float:\n        \"\"\"Evaluate bullet point organization\"\"\"\n        score = 0\n        num_bullets = len(content.bullets)\n        columns = layout.genes['bullet_columns']\n\n        # Optimal column usage\n        if num_bullets <= 4:\n            if columns == 1:\n                score += 10\n            else:\n                score -= 5\n        elif 5 <= num_bullets <= 8:\n            if columns == 2:\n                score += 12\n            else:\n                score -= 3\n        else:  # >8 bullets\n            if columns == 2:\n                score += 8\n            else:\n                score -= 10\n\n        # Bullet density penalty\n        avg_bullet_length = np.mean([len(bullet.split()) for bullet in content.bullets])\n        if avg_bullet_length > 15 and columns == 1:\n            score -= 8\n\n        # Too many bullets penalty\n        if num_bullets > 7:\n            score -= (num_bullets - 7) * 2\n\n        return score\n\n    def _evaluate_image_handling_advanced(self, layout: SlideLayout, content: SlideContent) -> float:\n        \"\"\"Advanced image handling evaluation using PIL image analysis\"\"\"\n        score = 0\n        img_analysis = content.image_analysis\n\n        # Basic image presence handling\n        if img_analysis['has_images'] and layout.genes['image_position'] != \"none\":\n            score += 15\n        elif not img_analysis['has_images'] and layout.genes['image_position'] == \"none\":\n            score += 10\n        elif not img_analysis['has_images'] and layout.genes['image_position'] != \"none\":\n            score -= 25\n        else:\n            score -= 15\n\n        # Advanced image-specific evaluations\n        if img_analysis['has_images']:\n            image_count = img_analysis['count']\n\n            # Image layout appropriateness\n            if image_count == 1:\n                if layout.genes['image_layout'] == \"single\":\n                    score += 10\n                else:\n                    score -= 5\n            elif image_count == 2:\n                if layout.genes['image_layout'] in [\"horizontal\", \"vertical\"]:\n                    score += 8\n                else:\n                    score -= 3\n            elif image_count >= 3:\n                if layout.genes['image_layout'] == \"grid\":\n                    score += 12\n                else:\n                    score -= 6\n\n            # Image size based on orientation and count\n            if img_analysis['dominant_orientation'] == 'landscape':\n                if layout.genes['image_position'] in [\"top\", \"bottom\"]:\n                    score += 6\n                elif layout.genes['image_position'] in [\"left\", \"right\"] and layout.genes['image_size'] != \"large\":\n                    score += 4\n            elif img_analysis['dominant_orientation'] == 'portrait':\n                if layout.genes['image_position'] in [\"left\", \"right\"]:\n                    score += 6\n                elif layout.genes['image_position'] in [\"top\", \"bottom\"] and layout.genes['image_size'] == \"small\":\n                    score += 4\n\n            # Mixed orientations penalty for simple layouts\n            if img_analysis['mixed_orientations'] and layout.genes['image_layout'] != \"grid\":\n                score -= 8\n\n            # Large images consideration\n            if img_analysis['has_large_images']:\n                if layout.genes['image_size'] == \"large\" and len(content.bullets) <= 3:\n                    score += 8\n                elif layout.genes['image_size'] == \"small\":\n                    score -= 6\n\n        return score\n\n    def _evaluate_visual_balance_advanced(self, layout: SlideLayout, content: SlideContent) -> float:\n        \"\"\"Enhanced visual balance considering actual image properties\"\"\"\n        score = 0\n        img_analysis = content.image_analysis\n\n        # Basic margin and spacing\n        margin = layout.genes['margin_size']\n        content_density = content.content_density\n\n        if content_density <= 2:\n            if margin in [\"medium\", \"large\"]:\n                score += 8\n        elif content_density <= 5:\n            if margin == \"medium\":\n                score += 10\n        else:\n            if margin == \"small\":\n                score += 6\n            else:\n                score -= 5\n\n        # Image-text balance based on actual content\n        balance = layout.genes['image_text_balance']\n        actual_ratio = content.text_image_ratio\n\n        if actual_ratio == \"text_only\" and balance == \"text_heavy\":\n            score += 10\n        elif actual_ratio == \"image_heavy\" and balance == \"image_heavy\":\n            score += 12\n        elif actual_ratio == \"balanced\" and balance == \"balanced\":\n            score += 15\n        else:\n            score -= 8\n\n        # Title position considering image presence and position\n        title_pos = layout.genes['title_position']\n        if img_analysis['has_images']:\n            img_pos = layout.genes['image_position']\n            if title_pos == \"top\" and img_pos in [\"left\", \"right\", \"bottom\"]:\n                score += 8\n            elif title_pos == \"center\" and img_pos in [\"top\", \"bottom\"]:\n                score += 6\n        else:\n            if title_pos == \"top\":\n                score += 8\n\n        return score\n\n    def _evaluate_color_contrast(self, layout: SlideLayout) -> float:\n        \"\"\"Evaluate color contrast for readability\"\"\"\n        score = 0\n\n        text_color = layout.genes['text_color']\n        bg_color = layout.genes['background_color']\n\n        contrast_key = (text_color, bg_color)\n        if contrast_key in self.color_contrast_map:\n            contrast_ratio = self.color_contrast_map[contrast_key]\n            if contrast_ratio >= 0.8:\n                score += 12\n            elif contrast_ratio >= 0.6:\n                score += 6\n            else:\n                score -= 15\n        else:\n            # Unknown combination, assume poor contrast\n            score -= 10\n\n        return score\n\n    def _evaluate_consistency(self, layout: SlideLayout, previous_layout: SlideLayout) -> float:\n        \"\"\"Evaluate consistency with previous slide\"\"\"\n        if previous_layout is None:\n            return 0\n\n        score = 0\n\n        # Theme consistency\n        if layout.genes['theme'] == previous_layout.genes['theme']:\n            score += 10\n\n        # Title position consistency\n        if layout.genes['title_position'] == previous_layout.genes['title_position']:\n            score += 5\n\n        # Color scheme consistency\n        if (layout.genes['background_color'] == previous_layout.genes['background_color'] and\n            layout.genes['text_color'] == previous_layout.genes['text_color']):\n            score += 6\n\n        return score\n\n    def _evaluate_content_appropriateness(self, layout: SlideLayout, content: SlideContent) -> float:\n        \"\"\"Evaluate if layout matches content type and density\"\"\"\n        score = 0\n\n        # Content density vs layout complexity\n        total_content = len(content.title.split()) + sum(len(b.split()) for b in content.bullets)\n\n        if total_content < 30:  # Light content\n            if layout.genes['margin_size'] in [\"medium\", \"large\"]:\n                score += 6\n        elif total_content > 80:  # Heavy content\n            if (layout.genes['bullet_columns'] == 2 and\n                layout.genes['margin_size'] == \"small\"):\n                score += 8\n            else:\n                score -= 5\n\n        # Title length vs title font size\n        title_words = len(content.title.split())\n        title_font = layout.genes['title_font_size']\n\n        if title_words > 6 and title_font > 36:\n            score -= 6  # Large font for long title\n        elif title_words <= 3 and title_font >= 32:\n            score += 4  # Good emphasis for short title\n\n        return score\n\n    def _evaluate_accessibility(self, layout: SlideLayout) -> float:\n        \"\"\"Evaluate accessibility considerations\"\"\"\n        score = 0\n\n        # Minimum font sizes for accessibility\n        if layout.genes['bullet_font_size'] >= 16:\n            score += 5\n        if layout.genes['title_font_size'] >= 28:\n            score += 3\n\n        # High contrast combinations get bonus\n        text_color = layout.genes['text_color']\n        bg_color = layout.genes['background_color']\n\n        high_contrast_pairs = [\n            (\"black\", \"white\"), (\"white\", \"dark_blue\"), (\"dark_gray\", \"white\")\n        ]\n\n        if (text_color, bg_color) in high_contrast_pairs:\n            score += 8\n\n        return score\n\n    def _evaluate_image_text_balance(self, layout: SlideLayout, content: SlideContent) -> float:\n        \"\"\"Evaluate the balance between images and text\"\"\"\n        score = 0\n        img_analysis = content.image_analysis\n\n        if not img_analysis['has_images']:\n            return 0\n\n        # Check if layout respects content type\n        balance_setting = layout.genes['image_text_balance']\n        actual_balance = content.text_image_ratio\n\n        # Reward matching balance settings\n        if actual_balance == \"text_heavy\" and balance_setting == \"text_heavy\":\n            score += 12\n        elif actual_balance == \"image_heavy\" and balance_setting == \"image_heavy\":\n            score += 12\n        elif actual_balance == \"balanced\" and balance_setting == \"balanced\":\n            score += 15\n        else:\n            score -= 6\n\n        # Consider image complexity\n        complexity = img_analysis['complexity_score']\n        if complexity > 3 and balance_setting != \"image_heavy\":\n            score -= 8\n        elif complexity < 1.5 and balance_setting == \"image_heavy\":\n            score -= 5\n\n        return score\n\n    def _evaluate_boundary_constraints(self, layout: SlideLayout, content: SlideContent) -> float:\n        \"\"\"Evaluate if content stays within slide boundaries\"\"\"\n        score = 0\n        penalties = 0\n\n        # Get layout parameters\n        margin = self.margin_sizes[layout.genes['margin_size']]\n\n        # Calculate usable area\n        usable_width = self.slide_width - (2 * margin)\n        usable_height = self.slide_height - (2 * margin)\n\n        # 1. Check title boundaries\n        title_penalty = self._check_title_boundaries(layout, content, margin, usable_width, usable_height)\n        penalties += title_penalty\n\n        # 2. Check bullet text boundaries\n        bullet_penalty = self._check_bullet_boundaries(layout, content, margin, usable_width, usable_height)\n        penalties += bullet_penalty\n\n        # 3. Check image boundaries\n        if content.image_analysis['has_images']:\n            image_penalty = self._check_image_boundaries(layout, content, margin, usable_width, usable_height)\n            penalties += image_penalty\n\n        # Apply penalties (negative score for boundary violations)\n        score -= penalties\n\n        # Bonus for good boundary management\n        if penalties == 0:\n            score += 10  # Perfect boundary adherence bonus\n        elif penalties < 5:\n            score += 5   # Minor violations only\n\n        return score\n\n    def _check_title_boundaries(self, layout: SlideLayout, content: SlideContent,\n                               margin: float, usable_width: float, usable_height: float) -> float:\n        \"\"\"Check if title stays within boundaries\"\"\"\n        penalty = 0\n\n        # Get title properties\n        title_font_size = layout.genes['title_font_size']\n        title_height = (title_font_size * self.font_height_ratio * self.points_to_inches)\n        title_chars = len(content.title)\n\n        # Estimate title width (rough approximation: avg 0.6 of height per character)\n        title_width = title_chars * (title_font_size * 0.6 * self.points_to_inches)\n\n        # Get title position\n        title_pos = layout.genes['title_position']\n        pos_data = self.title_positions[title_pos]\n\n        # Calculate actual position\n        title_x = margin + (pos_data['x'] * usable_width)\n        title_y = margin + (pos_data['y'] * usable_height)\n\n        # Check horizontal boundaries\n        if title_pos in ['center', 'top']:\n            # Centered text\n            left_edge = title_x - (title_width / 2)\n            right_edge = title_x + (title_width / 2)\n        elif title_pos == 'left':\n            left_edge = title_x\n            right_edge = title_x + title_width\n        else:  # right\n            left_edge = title_x - title_width\n            right_edge = title_x\n\n        # Check if title exceeds boundaries\n        if left_edge < margin:\n            penalty += abs(left_edge - margin) * 10  # 10 points per inch outside\n        if right_edge > (self.slide_width - margin):\n            penalty += abs(right_edge - (self.slide_width - margin)) * 10\n\n        # Check vertical boundaries\n        if title_y + title_height > (self.slide_height - margin):\n            penalty += abs(title_y + title_height - (self.slide_height - margin)) * 10\n\n        # Extra penalty for very long titles with large fonts\n        if title_width > usable_width * 0.9 and title_font_size > 36:\n            penalty += 15  # Encourage smaller font for long titles\n\n        return penalty\n\n    def _check_bullet_boundaries(self, layout: SlideLayout, content: SlideContent,\n                                margin: float, usable_width: float, usable_height: float) -> float:\n        \"\"\"Check if bullet points stay within boundaries\"\"\"\n        penalty = 0\n\n        if not content.bullets:\n            return 0\n\n        # Get bullet properties\n        bullet_font_size = layout.genes['bullet_font_size']\n        bullet_height = (bullet_font_size * self.font_height_ratio * self.points_to_inches)\n        num_columns = layout.genes['bullet_columns']\n\n        # Calculate space needed for bullets\n        bullets_per_column = len(content.bullets) / num_columns\n        total_bullet_height = bullets_per_column * bullet_height * 1.5  # 1.5 for line spacing\n\n        # Reserve space for title\n        title_space = 1.5  # inches\n        available_height = usable_height - title_space\n\n        # Check if bullets exceed vertical space\n        if total_bullet_height > available_height:\n            penalty += (total_bullet_height - available_height) * 15\n\n        # Check horizontal space for each bullet\n        column_width = usable_width / num_columns\n\n        for bullet in content.bullets:\n            # Estimate bullet width\n            bullet_chars = len(bullet)\n            bullet_width = bullet_chars * (bullet_font_size * 0.5 * self.points_to_inches)\n\n            # Check if bullet exceeds column width\n            if bullet_width > column_width * 0.95:  # 95% to leave some padding\n                penalty += (bullet_width - column_width * 0.95) * 8\n\n                # Extra penalty for very long bullets in single column\n                if num_columns == 1 and bullet_chars > 100:\n                    penalty += 10\n\n        return penalty\n\n    def _check_image_boundaries(self, layout: SlideLayout, content: SlideContent,\n                              margin: float, usable_width: float, usable_height: float) -> float:\n        \"\"\"Check if images stay within boundaries\"\"\"\n        penalty = 0\n\n        if not content.image_analysis['has_images']:\n            return 0\n\n        image_position = layout.genes['image_position']\n        image_size = layout.genes['image_size']\n        image_layout_type = layout.genes['image_layout']\n\n        if image_position == 'none':\n            return 0\n\n        # Get image area constraints\n        if image_position in self.image_positions:\n            pos_constraints = self.image_positions[image_position]\n        else:\n            pos_constraints = {'x': 0.1, 'y': 0.3, 'max_width': 0.8, 'max_height': 0.6}\n\n        # Calculate image dimensions based on size\n        size_ratio = self.image_size_ratios[image_size]\n\n        # Check based on layout type\n        if image_layout_type == 'single' and len(content.images) > 0:\n            # Single image\n            img = content.images[0]\n            img_props = ImageAnalyzer.get_image_properties(img)\n\n            # Calculate scaled dimensions\n            if image_position in ['left', 'right']:\n                max_img_width = usable_width * pos_constraints.get('max_width', 0.4)\n                max_img_height = usable_height * 0.7  # 70% of usable height\n\n                # Calculate width and height based on aspect ratio\n                if img_props['aspect_ratio'] > 1:  # Landscape\n                    img_width = max_img_width\n                    img_height = img_width / img_props['aspect_ratio']\n                    if img_height > max_img_height:\n                        img_height = max_img_height\n                        img_width = img_height * img_props['aspect_ratio']\n                else:  # Portrait\n                    img_height = max_img_height\n                    img_width = img_height * img_props['aspect_ratio']\n                    if img_width > max_img_width:\n                        img_width = max_img_width\n                        img_height = img_width / img_props['aspect_ratio']\n\n                # Apply size ratio\n                img_width *= size_ratio * 1.5  # Adjust based on size setting\n                img_height *= size_ratio * 1.5\n\n                # Check if exceeds boundary\n                if img_width > max_img_width:\n                    penalty += (img_width - max_img_width) * 12\n                if img_height > max_img_height:\n                    penalty += (img_height - max_img_height) * 12\n\n            elif image_position in ['top', 'bottom']:\n                max_img_width = usable_width * 0.8  # 80% of usable width\n                max_img_height = usable_height * pos_constraints.get('max_height', 0.4)\n\n                # Similar logic for top/bottom positions\n                if img_props['aspect_ratio'] > 1:  # Landscape\n                    img_width = max_img_width\n                    img_height = img_width / img_props['aspect_ratio']\n                    if img_height > max_img_height:\n                        img_height = max_img_height\n                        img_width = img_height * img_props['aspect_ratio']\n                else:  # Portrait\n                    img_height = max_img_height\n                    img_width = img_height * img_props['aspect_ratio']\n                    if img_width > max_img_width:\n                        img_width = max_img_width\n                        img_height = img_width / img_props['aspect_ratio']\n\n                # Apply size ratio\n                img_width *= size_ratio * 1.5\n                img_height *= size_ratio * 1.5\n\n                # Check if exceeds boundary\n                if img_width > max_img_width:\n                    penalty += (img_width - max_img_width) * 12\n                if img_height > max_img_height:\n                    penalty += (img_height - max_img_height) * 12\n\n        elif image_layout_type == 'grid' and content.images:\n            # Grid layout\n            num_images = len(content.images)\n            cols = min(int(num_images**0.5) + 1, 3)  # Estimate grid columns\n            rows = (num_images + cols - 1) // cols     # Ceiling division for rows\n\n            # Calculate available space\n            max_grid_width = usable_width * pos_constraints.get('max_width', 0.8)\n            max_grid_height = usable_height * pos_constraints.get('max_height', 0.6)\n\n            # Calculate per-image dimensions\n            cell_width = max_grid_width / cols\n            cell_height = max_grid_height / rows\n\n            # Calculate approximate image sizes\n            for img in content.images:\n                img_props = ImageAnalyzer.get_image_properties(img)\n\n                # Calculate dimensions based on aspect ratio and size\n                if img_props['aspect_ratio'] > 1:  # Landscape\n                    img_width = cell_width * 0.9  # 90% of cell width\n                    img_height = img_width / img_props['aspect_ratio']\n                else:  # Portrait\n                    img_height = cell_height * 0.9  # 90% of cell height\n                    img_width = img_height * img_props['aspect_ratio']\n\n                # Apply size ratio\n                img_width *= size_ratio\n                img_height *= size_ratio\n\n                # Check if any image exceeds its cell\n                if img_width > cell_width:\n                    penalty += (img_width - cell_width) * 8\n                if img_height > cell_height:\n                    penalty += (img_height - cell_height) * 8\n\n        elif image_layout_type in ['horizontal', 'vertical'] and len(content.images) > 1:\n            # Horizontal or vertical layout\n            num_images = len(content.images)\n\n            if image_layout_type == 'horizontal':\n                # Images side by side\n                max_total_width = usable_width * 0.9  # 90% of usable width\n                width_per_image = max_total_width / num_images\n                max_height = usable_height * 0.4  # 40% of usable height\n\n                for img in content.images:\n                    img_props = ImageAnalyzer.get_image_properties(img)\n\n                    # Calculate dimensions\n                    img_width = width_per_image * 0.9  # 90% of allocated width\n                    img_height = img_width / img_props['aspect_ratio']\n\n                    # Apply size ratio\n                    img_width *= size_ratio\n                    img_height *= size_ratio\n\n                    # Check vertical overflow\n                    if img_height > max_height:\n                        penalty += (img_height - max_height) * 10\n\n            else:  # vertical\n                # Images stacked\n                max_total_height = usable_height * 0.8  # 80% of usable height\n                height_per_image = max_total_height / num_images\n                max_width = usable_width * 0.4  # 40% of usable width\n\n                for img in content.images:\n                    img_props = ImageAnalyzer.get_image_properties(img)\n\n                    # Calculate dimensions\n                    img_height = height_per_image * 0.9  # 90% of allocated height\n                    img_width = img_height * img_props['aspect_ratio']\n\n                    # Apply size ratio\n                    img_width *= size_ratio\n                    img_height *= size_ratio\n\n                    # Check horizontal overflow\n                    if img_width > max_width:\n                        penalty += (img_width - max_width) * 10\n\n        # Apply extra penalty for large images with small margins\n        if image_size == 'large' and layout.genes['margin_size'] == 'small':\n            penalty += 8  # Discourage this combination\n\n        # Extra penalty for trying to fit too many images in a small area\n        if len(content.images) > 4 and image_size != 'small':\n            penalty += (len(content.images) - 4) * 5\n\n        return penalty\n\n\nclass GeneticAlgorithmLayoutOptimizer:\n    \"\"\"Enhanced GA class for optimizing slide layouts\"\"\"\n\n    def __init__(self, population_size: int = 50, generations: int = 100,\n                 mutation_rate: float = 0.1, elite_size: int = 10):\n        self.population_size = population_size\n        self.generations = generations\n        self.mutation_rate = mutation_rate\n        self.elite_size = elite_size\n        self.fitness_evaluator = FitnessEvaluator()\n        self.best_layouts_history = []\n\n    def optimize_slide_layouts(self, slides_content: List[SlideContent]) -> List[SlideLayout]:\n        \"\"\"Optimize layouts for multiple slides \"\"\"\n        optimized_layouts = []\n\n        for i, slide_content in enumerate(slides_content):\n            print(f\"Optimizing layout for slide {i+1}/{len(slides_content)}: '{slide_content.title[:40]}...'\")\n            print(f\"  Content: {len(slide_content.bullets)} bullets, {len(slide_content.images)} images\")\n\n            previous_layout = optimized_layouts[-1] if optimized_layouts else None\n            best_layout = self._optimize_single_slide(slide_content, previous_layout)\n            optimized_layouts.append(best_layout)\n\n            print(f\"  Best fitness: {best_layout.fitness_score:.2f}\")\n\n        return optimized_layouts\n\n    def _optimize_single_slide(self, content: SlideContent,\n                              previous_layout: SlideLayout = None) -> SlideLayout:\n        \"\"\"Optimize layout for a single slide\"\"\"\n\n        # Initialize population with image-aware constraints\n        population = self._initialize_smart_population(content)\n\n        best_fitness_history = []\n\n        for generation in range(self.generations):\n            # Evaluate fitness\n            for layout in population:\n                layout.fitness_score = self.fitness_evaluator.evaluate_fitness(\n                    layout, content, previous_layout\n                )\n\n            # Sort by fitness (descending)\n            population.sort(key=lambda x: x.fitness_score, reverse=True)\n\n            # Track progress\n            best_fitness_history.append(population[0].fitness_score)\n\n            if generation % 25 == 0:\n                print(f\"    Generation {generation}: Best fitness = {population[0].fitness_score:.2f}\")\n\n            # Create next generation\n            new_population = []\n\n            # Keep elite\n            new_population.extend(copy.deepcopy(population[:self.elite_size]))\n\n            # Generate offspring\n            while len(new_population) < self.population_size:\n                parent1 = self._tournament_selection(population)\n                parent2 = self._tournament_selection(population)\n\n                child = parent1.crossover(parent2)\n                child.mutate(self.mutation_rate)\n\n                # Apply image-specific constraints\n                self._apply_image_constraints(child, content)\n\n                new_population.append(child)\n\n            population = new_population\n\n        # Final evaluation\n        for layout in population:\n            layout.fitness_score = self.fitness_evaluator.evaluate_fitness(\n                layout, content, previous_layout\n            )\n\n        population.sort(key=lambda x: x.fitness_score, reverse=True)\n        return population[0]\n\n    def _initialize_smart_population(self, content: SlideContent) -> List[SlideLayout]:\n        \"\"\"Initialize population with image-aware heuristics\"\"\"\n        population = []\n        img_analysis = content.image_analysis\n\n        # Generate some layouts with smart defaults\n        smart_layouts_count = self.population_size // 3\n\n        for _ in range(smart_layouts_count):\n            layout = SlideLayout()\n\n            # Apply image-based heuristics\n            if img_analysis['has_images']:\n                if img_analysis['count'] == 1:\n                    layout.genes['image_layout'] = 'single'\n                    layout.genes['image_position'] = random.choice(['left', 'right', 'top'])\n                elif img_analysis['count'] == 2:\n                    layout.genes['image_layout'] = random.choice(['horizontal', 'vertical'])\n                elif img_analysis['count'] >= 3:\n                    layout.genes['image_layout'] = 'grid'\n                    layout.genes['image_position'] = 'grid'\n\n                # Set balance based on content analysis\n                layout.genes['image_text_balance'] = content.text_image_ratio\n\n                # Adjust image size based on orientation\n                if img_analysis['dominant_orientation'] == 'landscape':\n                    layout.genes['image_size'] = random.choice(['medium', 'large'])\n                else:\n                    layout.genes['image_size'] = random.choice(['small', 'medium'])\n            else:\n                layout.genes['image_position'] = 'none'\n                layout.genes['image_text_balance'] = 'text_heavy'\n\n            population.append(layout)\n\n        # Fill rest with random layouts\n        while len(population) < self.population_size:\n            population.append(SlideLayout())\n\n        return population\n\n    def _apply_image_constraints(self, layout: SlideLayout, content: SlideContent):\n        \"\"\"Apply constraints based on image analysis\"\"\"\n        img_analysis = content.image_analysis\n\n        # If no images, force image position to none\n        if not img_analysis['has_images']:\n            layout.genes['image_position'] = 'none'\n            layout.genes['image_text_balance'] = 'text_heavy'\n        else:\n            # Ensure image layout matches image count\n            if img_analysis['count'] == 1 and layout.genes['image_layout'] != 'single':\n                layout.genes['image_layout'] = 'single'\n            elif img_analysis['count'] >= 3 and layout.genes['image_layout'] not in ['grid']:\n                layout.genes['image_layout'] = 'grid'\n\n    def _tournament_selection(self, population: List[SlideLayout],\n                             tournament_size: int = 5) -> SlideLayout:\n        \"\"\"Tournament selection for parent selection\"\"\"\n        tournament = random.sample(population, min(tournament_size, len(population)))\n        return max(tournament, key=lambda x: x.fitness_score)\n\n    def get_top_layouts(self, content: SlideContent, top_k: int = 3,\n                       previous_layout: SlideLayout = None) -> List[SlideLayout]:\n        \"\"\"Get top K layout suggestions for a slide\"\"\"\n\n        population = self._initialize_smart_population(content)\n        population.extend([SlideLayout() for _ in range(self.population_size)])\n\n        for generation in range(self.generations // 2):\n            for layout in population:\n                layout.fitness_score = self.fitness_evaluator.evaluate_fitness(\n                    layout, content, previous_layout\n                )\n\n            population.sort(key=lambda x: x.fitness_score, reverse=True)\n            population = population[:self.population_size]\n\n            new_population = copy.deepcopy(population)\n\n            while len(new_population) < self.population_size * 2:\n                parent1 = self._tournament_selection(population)\n                parent2 = self._tournament_selection(population)\n                child = parent1.crossover(parent2)\n                child.mutate(self.mutation_rate)\n                self._apply_image_constraints(child, content)\n                new_population.append(child)\n\n            population = new_population\n\n        for layout in population:\n            layout.fitness_score = self.fitness_evaluator.evaluate_fitness(\n                layout, content, previous_layout\n            )\n\n        population.sort(key=lambda x: x.fitness_score, reverse=True)\n        return self._select_diverse_layouts(population, top_k)\n\n    def _select_diverse_layouts(self, population: List[SlideLayout],\n                               top_k: int) -> List[SlideLayout]:\n        \"\"\"Select diverse layouts from top performers\"\"\"\n        selected = [population[0]]\n\n        for layout in population[1:]:\n            if len(selected) >= top_k:\n                break\n\n            is_diverse = True\n            for selected_layout in selected:\n                if self._layouts_too_similar(layout, selected_layout):\n                    is_diverse = False\n                    break\n\n            if is_diverse:\n                selected.append(layout)\n\n        while len(selected) < top_k and len(selected) < len(population):\n            for layout in population:\n                if layout not in selected:\n                    selected.append(layout)\n                    break\n\n        return selected\n\n    def _layouts_too_similar(self, layout1: SlideLayout,\n                            layout2: SlideLayout, threshold: float = 0.8) -> bool:\n        \"\"\"Check if two layouts are too similar\"\"\"\n        same_genes = 0\n        total_genes = len(layout1.genes)\n\n        for gene_name in layout1.genes:\n            if layout1.genes[gene_name] == layout2.genes[gene_name]:\n                same_genes += 1\n\n        similarity = same_genes / total_genes\n        return similarity >= threshold\n\n\n\n\nclass PowerPointGenerator:\n    \"\"\"Generates PowerPoint slides from GA-optimized layouts\"\"\"\n\n    def __init__(self):\n        self.presentation = None\n        self.slide_width = Inches(10)\n        self.slide_height = Inches(7.5)\n\n        # Define layout dimensions (as fractions of slide dimensions)\n        self.layout_areas = {\n            'title': {'top': 0.05, 'left': 0.05, 'width': 0.9, 'height': 0.15},\n            'content': {'top': 0.25, 'left': 0.05, 'width': 0.9, 'height': 0.7},\n            'image_left': {'top': 0.25, 'left': 0.05, 'width': 0.4, 'height': 0.7},\n            'image_right': {'top': 0.25, 'left': 0.55, 'width': 0.4, 'height': 0.7},\n            'image_top': {'top': 0.25, 'left': 0.05, 'width': 0.9, 'height': 0.35},\n            'image_bottom': {'top': 0.6, 'left': 0.05, 'width': 0.9, 'height': 0.35},\n            'text_left': {'top': 0.25, 'left': 0.05, 'width': 0.45, 'height': 0.7},\n            'text_right': {'top': 0.25, 'left': 0.5, 'width': 0.45, 'height': 0.7},\n            'text_top': {'top': 0.25, 'left': 0.05, 'width': 0.9, 'height': 0.35},\n            'text_bottom': {'top': 0.6, 'left': 0.05, 'width': 0.9, 'height': 0.35}\n        }\n\n        # Color mappings\n        self.color_map = {\n            'black': RGBColor(0, 0, 0),\n            'white': RGBColor(255, 255, 255),\n            'dark_gray': RGBColor(64, 64, 64),\n            'light_gray': RGBColor(240, 240, 240),\n            'blue': RGBColor(0, 112, 192),\n            'dark_blue': RGBColor(0, 32, 96),\n            'light_blue': RGBColor(173, 216, 230)\n        }\n\n        # Margin size mappings (in inches)\n        self.margin_sizes = {\n            'small': 0.3,\n            'medium': 0.5,\n            'large': 0.8\n        }\n\n    def create_presentation_from_layouts(self, slides_content: List[SlideContent],\n                                       optimized_layouts: List[SlideLayout],\n                                       output_path: str = \"optimized_presentation.pptx\") -> str:\n        \"\"\"Create a complete PowerPoint presentation from optimized layouts\"\"\"\n\n        print(f\"🎯 Generating PowerPoint presentation with {len(slides_content)} slides...\")\n\n        # Create new presentation\n        self.presentation = Presentation()\n\n        # Remove default slide layout\n        if len(self.presentation.slides) > 0:\n            slide_to_remove = self.presentation.slides[0]\n            rId = self.presentation.slides._sldIdLst[0].rId\n            self.presentation.part.drop_rel(rId)\n            del self.presentation.slides._sldIdLst[0]\n\n        # Generate each slide\n        for i, (content, layout) in enumerate(zip(slides_content, optimized_layouts)):\n            print(f\"  📄 Creating slide {i+1}: '{content.title[:40]}...'\")\n            self._create_slide(content, layout)\n\n        # Save presentation\n        self.presentation.save(output_path)\n        print(f\"✅ Presentation saved as: {output_path}\")\n\n        return output_path\n\n    def _create_slide(self, content: SlideContent, layout: SlideLayout):\n        \"\"\"Create a single slide based on content and optimized layout\"\"\"\n\n        # Add blank slide\n        blank_slide_layout = self.presentation.slide_layouts[6]  # Blank layout\n        slide = self.presentation.slides.add_slide(blank_slide_layout)\n\n        # Apply background color\n        self._apply_background(slide, layout)\n\n        # Add title\n        self._add_title(slide, content, layout)\n\n        # Add content based on layout\n        if content.images:\n            self._add_content_with_images(slide, content, layout)\n        else:\n            self._add_text_only_content(slide, content, layout)\n\n        return slide\n\n    def _apply_background(self, slide, layout: SlideLayout):\n        \"\"\"Apply background color to slide\"\"\"\n        bg_color = layout.genes['background_color']\n\n        if bg_color != 'white':  # Default is white\n            background = slide.background\n            fill = background.fill\n            fill.solid()\n            fill.fore_color.rgb = self.color_map[bg_color]\n\n    def _add_title(self, slide, content: SlideContent, layout: SlideLayout):\n        \"\"\"Add title to slide based on layout specifications\"\"\"\n\n        title_pos = layout.genes['title_position']\n        title_font_size = layout.genes['title_font_size']\n        text_color = layout.genes['text_color']\n        margin = self.margin_sizes[layout.genes['margin_size']]\n\n        # Determine title position and size\n        if title_pos == 'top':\n            left = Inches(margin)\n            top = Inches(margin)\n            width = self.slide_width - Inches(2 * margin)\n            height = Inches(1.2)\n            alignment = PP_ALIGN.LEFT\n        elif title_pos == 'center':\n            left = Inches(margin)\n            top = Inches(2.5)\n            width = self.slide_width - Inches(2 * margin)\n            height = Inches(1.2)\n            alignment = PP_ALIGN.CENTER\n        elif title_pos == 'left':\n            left = Inches(margin)\n            top = Inches(margin)\n            width = self.slide_width * 0.4\n            height = Inches(1.2)\n            alignment = PP_ALIGN.LEFT\n        else:  # right\n            left = self.slide_width * 0.6\n            top = Inches(margin)\n            width = self.slide_width * 0.35\n            height = Inches(1.2)\n            alignment = PP_ALIGN.RIGHT\n\n        # Add title textbox\n        title_box = slide.shapes.add_textbox(left, top, width, height)\n        title_frame = title_box.text_frame\n        title_frame.clear()\n        title_frame.margin_left = Inches(0.1)\n        title_frame.margin_right = Inches(0.1)\n        title_frame.margin_top = Inches(0.1)\n        title_frame.margin_bottom = Inches(0.1)\n\n        # Add title text\n        p = title_frame.paragraphs[0]\n        p.text = content.title\n        p.alignment = alignment\n\n        # Format title\n        font = p.font\n        font.size = Pt(title_font_size)\n        font.bold = True\n        font.color.rgb = self.color_map[text_color]\n\n        return title_box\n\n    def _add_text_only_content(self, slide, content: SlideContent, layout: SlideLayout):\n        \"\"\"Add text content when no images are present\"\"\"\n\n        bullet_columns = layout.genes['bullet_columns']\n        bullet_font_size = layout.genes['bullet_font_size']\n        text_color = layout.genes['text_color']\n        margin = self.margin_sizes[layout.genes['margin_size']]\n\n        # Calculate content area\n        content_top = Inches(1.8)  # Below title\n        content_left = Inches(margin)\n        content_width = self.slide_width - Inches(2 * margin)\n        content_height = self.slide_height - content_top - Inches(margin)\n\n        if bullet_columns == 1:\n            # Single column\n            self._add_bullet_textbox(\n                slide, content.bullets,\n                content_left, content_top, content_width, content_height,\n                bullet_font_size, text_color\n            )\n        else:\n            # Two columns\n            col_width = (content_width - Inches(0.5)) / 2\n            mid_point = len(content.bullets) // 2\n\n            # Left column\n            left_bullets = content.bullets[:mid_point]\n            self._add_bullet_textbox(\n                slide, left_bullets,\n                content_left, content_top, col_width, content_height,\n                bullet_font_size, text_color\n            )\n\n            # Right column\n            right_bullets = content.bullets[mid_point:]\n            self._add_bullet_textbox(\n                slide, right_bullets,\n                content_left + col_width + Inches(0.5), content_top, col_width, content_height,\n                bullet_font_size, text_color\n            )\n\n    def _add_content_with_images(self, slide, content: SlideContent, layout: SlideLayout):\n        \"\"\"Add content with images based on layout specifications\"\"\"\n\n        image_position = layout.genes['image_position']\n        image_layout = layout.genes['image_layout']\n        image_size = layout.genes['image_size']\n        bullet_font_size = layout.genes['bullet_font_size']\n        text_color = layout.genes['text_color']\n        margin = self.margin_sizes[layout.genes['margin_size']]\n\n        if image_position == 'grid':\n            self._add_grid_layout(slide, content, layout)\n        elif image_position in ['left', 'right']:\n            self._add_side_by_side_layout(slide, content, layout)\n        elif image_position in ['top', 'bottom']:\n            self._add_stacked_layout(slide, content, layout)\n        else:\n            # Fallback to text-only\n            self._add_text_only_content(slide, content, layout)\n\n    def _add_side_by_side_layout(self, slide, content: SlideContent, layout: SlideLayout):\n        \"\"\"Add side-by-side image and text layout\"\"\"\n\n        image_position = layout.genes['image_position']\n        bullet_font_size = layout.genes['bullet_font_size']\n        text_color = layout.genes['text_color']\n        margin = self.margin_sizes[layout.genes['margin_size']]\n\n        content_top = Inches(1.8)\n        content_height = self.slide_height - content_top - Inches(margin)\n\n        # Determine image and text areas based on image size\n        size_ratios = {'small': 0.3, 'medium': 0.45, 'large': 0.6}\n        img_ratio = size_ratios[layout.genes['image_size']]\n        text_ratio = 1 - img_ratio - 0.05  # 5% gap\n\n        if image_position == 'left':\n            # Image on left, text on right\n            img_left = Inches(margin)\n            img_width = self.slide_width * img_ratio\n\n            text_left = img_left + img_width + Inches(0.3)\n            text_width = self.slide_width * text_ratio\n        else:  # right\n            # Text on left, image on right\n            text_left = Inches(margin)\n            text_width = self.slide_width * text_ratio\n\n            img_left = text_left + text_width + Inches(0.3)\n            img_width = self.slide_width * img_ratio\n\n        # Add images\n        self._add_images_to_area(slide, content.images, img_left, content_top,\n                               img_width, content_height, layout)\n\n        # Add text\n        self._add_bullet_textbox(slide, content.bullets, text_left, content_top,\n                               text_width, content_height, bullet_font_size, text_color)\n\n    def _add_stacked_layout(self, slide, content: SlideContent, layout: SlideLayout):\n        \"\"\"Add stacked (top/bottom) image and text layout\"\"\"\n\n        image_position = layout.genes['image_position']\n        bullet_font_size = layout.genes['bullet_font_size']\n        text_color = layout.genes['text_color']\n        margin = self.margin_sizes[layout.genes['margin_size']]\n\n        content_top = Inches(1.8)\n        content_width = self.slide_width - Inches(2 * margin)\n        total_height = self.slide_height - content_top - Inches(margin)\n\n        # Determine image and text areas\n        size_ratios = {'small': 0.3, 'medium': 0.5, 'large': 0.7}\n        img_ratio = size_ratios[layout.genes['image_size']]\n        text_ratio = 1 - img_ratio - 0.05\n\n        if image_position == 'top':\n            # Image on top, text below\n            img_top = content_top\n            img_height = total_height * img_ratio\n\n            text_top = img_top + img_height + Inches(0.2)\n            text_height = total_height * text_ratio\n        else:  # bottom\n            # Text on top, image below\n            text_top = content_top\n            text_height = total_height * text_ratio\n\n            img_top = text_top + text_height + Inches(0.2)\n            img_height = total_height * img_ratio\n\n        # Add images\n        self._add_images_to_area(slide, content.images, Inches(margin), img_top,\n                               content_width, img_height, layout)\n\n        # Add text\n        self._add_bullet_textbox(slide, content.bullets, Inches(margin), text_top,\n                               content_width, text_height, bullet_font_size, text_color)\n\n    def _add_grid_layout(self, slide, content: SlideContent, layout: SlideLayout):\n        \"\"\"Add grid layout for multiple images\"\"\"\n\n        bullet_font_size = layout.genes['bullet_font_size']\n        text_color = layout.genes['text_color']\n        margin = self.margin_sizes[layout.genes['margin_size']]\n\n        content_top = Inches(1.8)\n        content_width = self.slide_width - Inches(2 * margin)\n        content_height = self.slide_height - content_top - Inches(margin)\n\n        # Split area between images and text\n        if len(content.images) >= 3:\n            img_height = content_height * 0.6\n            text_height = content_height * 0.35\n\n            # Images in grid at top\n            self._add_images_to_area(slide, content.images, Inches(margin), content_top,\n                                   content_width, img_height, layout)\n\n            # Text at bottom\n            text_top = content_top + img_height + Inches(0.2)\n            self._add_bullet_textbox(slide, content.bullets, Inches(margin), text_top,\n                                   content_width, text_height, bullet_font_size, text_color)\n        else:\n            # Fallback to side-by-side for fewer images\n            self._add_side_by_side_layout(slide, content, layout)\n\n    def _add_images_to_area(self, slide, images: List[Image.Image], left, top, width, height, layout: SlideLayout):\n        \"\"\"Add PIL images to specified area\"\"\"\n\n        if not images:\n            return\n\n        image_layout = layout.genes['image_layout']\n\n        if len(images) == 1:\n            self._add_single_image(slide, images[0], left, top, width, height)\n        elif len(images) == 2:\n            if image_layout == 'horizontal':\n                self._add_two_images_horizontal(slide, images, left, top, width, height)\n            else:  # vertical\n                 self._add_two_images_vertical(slide, images, left, top, width, height)\n        else:  # 3+ images\n            self._add_multiple_images_grid(slide, images, left, top, width, height)\n\n    def _add_single_image(self, slide, image: Image.Image, left, top, width, height):\n        \"\"\"Add a single image with aspect ratio preservation\"\"\"\n\n        # Save PIL image to temporary file\n        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:\n            image.save(temp_file.name, 'PNG')\n            temp_path = temp_file.name\n\n        try:\n            # Calculate dimensions preserving aspect ratio\n            img_width, img_height = image.size\n            aspect_ratio = img_width / img_height\n\n            # Fit image within available space\n            if width / height > aspect_ratio:\n                # Height is limiting factor\n                final_height = height\n                final_width = height * aspect_ratio\n                img_left = left + (width - final_width) / 2\n                img_top = top\n            else:\n                # Width is limiting factor\n                final_width = width\n                final_height = width / aspect_ratio\n                img_left = left\n                img_top = top + (height - final_height) / 2\n\n            # Add image to slide\n            slide.shapes.add_picture(temp_path, img_left, img_top, final_width, final_height)\n\n        finally:\n            # Clean up temporary file\n            try:\n                os.unlink(temp_path)\n            except:\n                pass\n\n    def _add_two_images_horizontal(self, slide, images: List[Image.Image], left, top, width, height):\n        \"\"\"Add two images side by side\"\"\"\n\n        img_width = (width - Inches(0.2)) / 2  # 0.2\" gap between images\n\n        # Add first image\n        self._add_single_image(slide, images[0], left, top, img_width, height)\n\n        # Add second image\n        self._add_single_image(slide, images[1], left + img_width + Inches(0.2), top, img_width, height)\n\n    def _add_two_images_vertical(self, slide, images: List[Image.Image], left, top, width, height):\n        \"\"\"Add two images stacked vertically\"\"\"\n\n        img_height = (height - Inches(0.2)) / 2  # 0.2\" gap between images\n\n        # Add first image\n        self._add_single_image(slide, images[0], left, top, width, img_height)\n\n        # Add second image\n        self._add_single_image(slide, images[1], left, top + img_height + Inches(0.2), width, img_height)\n\n    def _add_multiple_images_grid(self, slide, images: List[Image.Image], left, top, width, height):\n        \"\"\"Add multiple images in a grid layout\"\"\"\n\n        num_images = len(images)\n\n        # Determine grid layout\n        if num_images <= 4:\n            cols = 2\n            rows = (num_images + 1) // 2\n        elif num_images <= 6:\n            cols = 3\n            rows = 2\n        else:\n            cols = 3\n            rows = 3\n\n        # Calculate image dimensions\n        gap = Inches(0.1)\n        img_width = (width - gap * (cols - 1)) / cols\n        img_height = (height - gap * (rows - 1)) / rows\n\n        # Add images to grid\n        for i, image in enumerate(images[:rows * cols]):\n            row = i // cols\n            col = i % cols\n\n            img_left = left + col * (img_width + gap)\n            img_top = top + row * (img_height + gap)\n\n            self._add_single_image(slide, image, img_left, img_top, img_width, img_height)\n\n    def _add_bullet_textbox(self, slide, bullets: List[str], left, top, width, height,\n                       font_size: int, text_color: str):\n      \"\"\"Add bullet points in a text box with simple bullet formatting\"\"\"\n\n      if not bullets:\n          return\n\n      # Add textbox\n      textbox = slide.shapes.add_textbox(left, top, width, height)\n      text_frame = textbox.text_frame\n      text_frame.clear()\n      text_frame.margin_left = Inches(0.2)\n      text_frame.margin_right = Inches(0.1)\n      text_frame.margin_top = Inches(0.1)\n      text_frame.margin_bottom = Inches(0.1)\n      text_frame.word_wrap = True\n\n      # Add bullets with simple formatting\n      for i, bullet_text in enumerate(bullets):\n          if i == 0:\n              p = text_frame.paragraphs[0]\n          else:\n              p = text_frame.add_paragraph()\n\n          # Add bullet character explicitly\n          p.text = f\"• {bullet_text}\"\n          p.alignment = PP_ALIGN.LEFT\n\n          # Format font\n          font = p.font\n          font.size = Pt(font_size)\n          font.color.rgb = self.color_map[text_color]\n          font.name = 'Calibri'\n\n      return textbox\n\n\nclass SlideTemplateManager:\n    \"\"\"Manages different slide templates and themes\"\"\"\n\n    def __init__(self):\n        self.templates = {\n            'default': {\n                'master_background': 'white',\n                'accent_color': 'blue',\n                'title_font': 'Calibri',\n                'body_font': 'Calibri'\n            },\n            'dark': {\n                'master_background': 'dark_blue',\n                'accent_color': 'light_blue',\n                'title_font': 'Calibri',\n                'body_font': 'Calibri'\n            },\n            'modern': {\n                'master_background': 'light_gray',\n                'accent_color': 'dark_gray',\n                'title_font': 'Segoe UI',\n                'body_font': 'Segoe UI'\n            },\n            'light': {\n                'master_background': 'white',\n                'accent_color': 'dark_gray',\n                'title_font': 'Arial',\n                'body_font': 'Arial'\n            }\n        }\n\n    def apply_template(self, presentation: Presentation, template_name: str):\n        \"\"\"Apply a template to the presentation\"\"\"\n        if template_name not in self.templates:\n            template_name = 'default'\n\n        template = self.templates[template_name]\n\n        # Apply template-specific formatting\n        # This would involve modifying the slide master, but for simplicity\n        # we'll handle this at the individual slide level\n        return template\n\n\n\nclass IntegratedPresentationGenerator:\n    \"\"\"Main class that integrates GA optimization with PowerPoint generation\"\"\"\n\n    def __init__(self, ga_params: Dict[str, Any] = None):\n        self.ga_optimizer = GeneticAlgorithmLayoutOptimizer(\n            **(ga_params or {\n                'population_size': 40,\n                'generations': 60,\n                'mutation_rate': 0.12,\n                'elite_size': 6\n            })\n        )\n        self.ppt_generator = PowerPointGenerator()\n        self.template_manager = SlideTemplateManager()\n\n    def generate_presentation_from_content(self, slides_data: List[Dict[str, Any]],\n                                         output_path: str = \"ga_optimized_presentation.pptx\",\n                                         template: str = \"default\") -> Dict[str, Any]:\n        \"\"\"Complete pipeline: content -> GA optimization -> PowerPoint generation\"\"\"\n\n        print(\"🚀 Starting Integrated Presentation Generation Pipeline\")\n        print(\"=\" * 60)\n\n        # Step 1: Convert input data to SlideContent objects\n        print(\"📊 Step 1: Processing slide content...\")\n        slides_content = []\n        for i, slide_data in enumerate(slides_data):\n            content = SlideContent(\n                title=slide_data['title'],\n                bullets=slide_data['bullets'],\n                images=slide_data.get('images', [])\n            )\n            slides_content.append(content)\n            print(f\"  Slide {i+1}: {len(content.bullets)} bullets, {len(content.images)} images\")\n\n        # Step 2: Optimize layouts using GA\n        print(f\"\\n🧬 Step 2: Optimizing layouts with Genetic Algorithm...\")\n        optimized_layouts = self.ga_optimizer.optimize_slide_layouts(slides_content)\n\n        # Step 3: Generate PowerPoint presentation\n        print(f\"\\n📋 Step 3: Generating PowerPoint presentation...\")\n        ppt_path = self.ppt_generator.create_presentation_from_layouts(\n            slides_content, optimized_layouts, output_path\n        )\n\n        print(f\"\\n✅ Pipeline Complete!\")\n        print(f\"📄 PowerPoint file: {ppt_path}\")\n\n        return {\n            'ppt_path': ppt_path,\n            'optimized_layouts': optimized_layouts,\n        }","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-06-12T22:46:47.032422Z","iopub.execute_input":"2025-06-12T22:46:47.032646Z","iopub.status.idle":"2025-06-12T22:46:47.157080Z","shell.execute_reply.started":"2025-06-12T22:46:47.032629Z","shell.execute_reply":"2025-06-12T22:46:47.156308Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"#### GA Calling Function","metadata":{}},{"cell_type":"code","source":"def GA_pptx_generation(slides:list[Dict],output_path:str):\n    demo_slides = slides\n    generator = IntegratedPresentationGenerator({\n        'population_size': 500,\n        'generations': 100,\n        'mutation_rate': 0.12,\n        'elite_size': 10\n    })\n    results = generator.generate_presentation_from_content(\n        demo_slides,\n        output_path=output_path # Write here your output path\n    )\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:46:47.159491Z","iopub.execute_input":"2025-06-12T22:46:47.159920Z","iopub.status.idle":"2025-06-12T22:46:47.163947Z","shell.execute_reply.started":"2025-06-12T22:46:47.159900Z","shell.execute_reply":"2025-06-12T22:46:47.163181Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Endpoints using FastAPI","metadata":{}},{"cell_type":"code","source":"!pip install fastapi uvicorn pyngrok nest_asyncio python-multipart","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:46:47.165014Z","iopub.execute_input":"2025-06-12T22:46:47.165385Z","iopub.status.idle":"2025-06-12T22:46:49.239778Z","shell.execute_reply.started":"2025-06-12T22:46:47.165359Z","shell.execute_reply":"2025-06-12T22:46:49.238976Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.9)\nRequirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.3)\nRequirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\nRequirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\nRequirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.45.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (4.9.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import nest_asyncio\nnest_asyncio.apply()\n\nfrom fastapi import FastAPI, File, UploadFile, Query,Path, Body\nfrom pyngrok import ngrok\nfrom typing import Dict, List,Any\nimport uvicorn\nimport shutil\nfrom tempfile import NamedTemporaryFile\nimport base64\nfrom io import BytesIO\nfrom fastapi.responses import JSONResponse, FileResponse\nfrom PIL import Image\n\napp = FastAPI()\n\nimport uuid\n\n# Global in-memory store for demo (replace with a real DB or persistent storage)\nDOCUMENT_STORE = {}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:55:19.596747Z","iopub.execute_input":"2025-06-12T22:55:19.597094Z","iopub.status.idle":"2025-06-12T22:55:19.632041Z","shell.execute_reply.started":"2025-06-12T22:55:19.597068Z","shell.execute_reply":"2025-06-12T22:55:19.631210Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# add the file and save the processed data of the file in DOCUMENT_STORE with uuid\n@app.post(\"/process_document\")\nasync def process_document(file: UploadFile = File(...)):\n    with NamedTemporaryFile(delete=False, suffix=f\"_{file.filename}\") as tmp:\n        shutil.copyfileobj(file.file, tmp)\n        temp_file_path = tmp.name\n\n    doc_id = str(uuid.uuid4())\n\n    # Store file path and filename in DOCUMENT_STORE\n    DOCUMENT_STORE[doc_id] = {\n        \"filename\": file.filename,\n        \"filepath\": temp_file_path\n    }\n\n    # Process document normally\n    doc_pipeline = DocumentProcessingPipeline(temp_file_path,\"semantic\")\n    results = doc_pipeline.run()\n\n    # Cache results in DOCUMENT_STORE so you can reuse embeddings & titles\n    DOCUMENT_STORE[doc_id][\"results\"] = results\n\n    return {\n        \"doc_id\": doc_id,\n        \"titles\": results[\"titles\"],\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:55:20.266779Z","iopub.execute_input":"2025-06-12T22:55:20.267078Z","iopub.status.idle":"2025-06-12T22:55:20.274491Z","shell.execute_reply.started":"2025-06-12T22:55:20.267055Z","shell.execute_reply":"2025-06-12T22:55:20.273583Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"@app.get(\"/slides/{doc_id}\")\nasync def get_slides_json(\n    doc_id: str = Path(..., title=\"The ID of the document\"),\n    titles: Dict[str, List[int]] = Body(...)\n):\n    if doc_id not in DOCUMENT_STORE:\n        return JSONResponse(status_code=404, content={\"error\": \"Document ID not found\"})\n\n    results = DOCUMENT_STORE[doc_id][\"results\"]\n    selected_indices = titles.get(\"titles\", [])\n\n    # slide_builder = SlideBuilder(\n#     titles=[title.capitalize() for title in results[\"titles\"]],\n#     # titles=results[\"constructed_queries\"],\n#     lines=results[\"lines\"],\n#     images=results[\"images\"],\n#     db_manager=results[\"db_manager\"],\n#     # text_embedder = results[\"text_embedder\"],\n#     title_embeddings = results[\"title_embeddings\"],\n#     summarizer = results[\"summarizer\"],\n#     # title_embeddings = results[\"text_embedder\"].encode(results[\"constructed_queries\"]),\n# )\n\n# slides = slide_builder.build_slides(summarization_strategy=\"cot\", max_image_occurrences=1)\n    selected_titles = [results[\"titles\"][i] for i in selected_indices]\n\n    slide_builder = SlideBuilder(\n        titles=[title.capitalize() for title in selected_titles],\n        # titles=results[\"constructed_queries\"],\n        lines=results[\"lines\"],\n        images=results[\"images\"],\n        db_manager=results[\"db_manager\"],\n        # text_embedder = results[\"text_embedder\"],\n        title_embeddings = results[\"title_embeddings\"],\n        summarizer = results[\"summarizer\"],\n        # title_embeddings = results[\"text_embedder\"].encode(results[\"constructed_queries\"]),\n    )\n\n    # slides = slide_builder.build_slides()\n    slides = slide_builder.build_slides(summarization_strategy=\"cot\", max_image_occurrences=1)\n    # print(slides)\n    for slide in slides:\n        serialized_images = []\n        for img in slide.get(\"images\", []):\n            if isinstance(img, Image.Image):\n                buf = io.BytesIO()\n                img.save(buf, format=\"PNG\")\n                image_b64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n                serialized_images.append(image_b64)\n            else:\n                serialized_images.append(None)\n        slide[\"images\"] = serialized_images\n\n    return JSONResponse(content={\"slides\": slides})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:55:20.830422Z","iopub.execute_input":"2025-06-12T22:55:20.831027Z","iopub.status.idle":"2025-06-12T22:55:20.839413Z","shell.execute_reply.started":"2025-06-12T22:55:20.831003Z","shell.execute_reply":"2025-06-12T22:55:20.838496Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"@app.post(\"/Defualt_generate_pptx/{doc_id}\")\nasync def generate_pptx_from_slides(\n    doc_id: str = Path(..., title=\"The ID of the document\"),\n    slides: List[Dict[str, Any]] = Body(...)\n):\n    if doc_id not in DOCUMENT_STORE:\n        return JSONResponse(status_code=404, content={\"error\": \"Document ID not found\"})\n\n    # Decode base64 images back to PIL.Image objects\n    for slide in slides:\n        if \"images\" in slide and isinstance(slide[\"images\"], list):\n            decoded_images = []\n            for img_b64 in slide[\"images\"]:\n                if isinstance(img_b64, str):\n                    img_bytes = base64.b64decode(img_b64)\n                    img = Image.open(io.BytesIO(img_bytes))\n                    decoded_images.append(img)\n                else:\n                    decoded_images.append(None)\n            slide[\"images\"] = decoded_images\n\n    file_name = DOCUMENT_STORE[doc_id][\"filename\"].replace(\".pdf\", \"\")\n    file_path = f\"/kaggle/working/{file_name}.pptx\"\n    DOCUMENT_STORE[doc_id][\"defualt_pptx_path\"] = file_path\n    \n    # calling the builder\n    builder = PresentationBuilder(\n            title=file_name,\n            font_name=\"Arial\",\n            font_size=20,\n            # template_path=\"/kaggle/working/template_2.pptx\"\n        )\n    \n    # build from list takes [{title,images,bulltes},{title,images,bulltes},{title,images,bulltes}]\n    builder.build_from_list(slides)\n    # saving the file\n    builder.save(file_path)\n    \n    # ensure that the file saved successfully to return it.\n    if not os.path.exists(file_path):\n        return JSONResponse(status_code=500, content={\"error\": \"Failed to generate PowerPoint\"})\n    \n    return FileResponse(\n        path=file_path,\n        media_type=\"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n        filename=f\"{file_name}.pptx\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:55:21.094798Z","iopub.execute_input":"2025-06-12T22:55:21.095097Z","iopub.status.idle":"2025-06-12T22:55:21.104380Z","shell.execute_reply.started":"2025-06-12T22:55:21.095074Z","shell.execute_reply":"2025-06-12T22:55:21.103402Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"@app.post(\"/GA_generate_pptx/{doc_id}\")\nasync def generate_pptx_from_slides(\n    doc_id: str = Path(..., title=\"The ID of the document\"),\n    slides: List[Dict[str, Any]] = Body(...)\n):\n    if doc_id not in DOCUMENT_STORE:\n        return JSONResponse(status_code=404, content={\"error\": \"Document ID not found\"})\n\n    # Decode base64 images back to PIL.Image objects\n    for slide in slides:\n        if \"images\" in slide and isinstance(slide[\"images\"], list):\n            decoded_images = []\n            for img_b64 in slide[\"images\"]:\n                if isinstance(img_b64, str):\n                    img_bytes = base64.b64decode(img_b64)\n                    img = Image.open(io.BytesIO(img_bytes))\n                    decoded_images.append(img)\n                else:\n                    decoded_images.append(None)\n            slide[\"images\"] = decoded_images\n\n    file_name = DOCUMENT_STORE[doc_id][\"filename\"].replace(\".pdf\", \"\")\n    file_path = f\"/kaggle/working/{file_name}.pptx\"\n    DOCUMENT_STORE[doc_id][\"ga_pptx_path\"] = file_path\n\n    GA_pptx_generation(slides, file_path)\n\n    if not os.path.exists(file_path):\n        return JSONResponse(status_code=500, content={\"error\": \"Failed to generate PowerPoint\"})\n\n    return FileResponse(\n        path=file_path,\n        media_type=\"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n        filename=f\"{file_name}.pptx\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:55:21.313288Z","iopub.execute_input":"2025-06-12T22:55:21.313586Z","iopub.status.idle":"2025-06-12T22:55:21.321551Z","shell.execute_reply.started":"2025-06-12T22:55:21.313565Z","shell.execute_reply":"2025-06-12T22:55:21.320600Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"@app.get(\"/defualt_doc/{doc_id}\")\nasync def download_pptx( doc_id: str = Path(..., title=\"The ID of the document\")):\n    if doc_id not in DOCUMENT_STORE:\n        return JSONResponse(status_code=404, content={\"error\": \"Document ID not found\"})\n    file_path=DOCUMENT_STORE[doc_id][\"defualt_pptx_path\"]\n    if not os.path.exists(file_path):\n        return JSONResponse(status_code=404, content={\"error\": \"File not found\"})\n    file_name = DOCUMENT_STORE[doc_id][\"filename\"].replace(\".pdf\",\"\")\n    return FileResponse(\n        path=file_path,\n        media_type=\"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n        filename=f\"{file_name}.pptx\"\n    )\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:55:24.599671Z","iopub.execute_input":"2025-06-12T22:55:24.600316Z","iopub.status.idle":"2025-06-12T22:55:24.606140Z","shell.execute_reply.started":"2025-06-12T22:55:24.600289Z","shell.execute_reply":"2025-06-12T22:55:24.605182Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"@app.get(\"/ga_doc/{doc_id}\")\nasync def download_pptx( doc_id: str = Path(..., title=\"The ID of the document\")):\n    if doc_id not in DOCUMENT_STORE:\n        return JSONResponse(status_code=404, content={\"error\": \"Document ID not found\"})\n    file_path=DOCUMENT_STORE[doc_id][\"ga_pptx_path\"]\n    if not os.path.exists(file_path):\n        return JSONResponse(status_code=404, content={\"error\": \"File not found\"})\n    file_name = DOCUMENT_STORE[doc_id][\"filename\"].replace(\".pdf\",\"\")\n    return FileResponse(\n        path=file_path,\n        media_type=\"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n        filename=f\"{file_name}.pptx\"\n    )\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T22:55:24.842489Z","iopub.execute_input":"2025-06-12T22:55:24.842829Z","iopub.status.idle":"2025-06-12T22:55:24.849445Z","shell.execute_reply.started":"2025-06-12T22:55:24.842803Z","shell.execute_reply":"2025-06-12T22:55:24.848577Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"\nimport requests\nimport json\ndef update_gist(url):\n    gist_id = \"79ec449dc37d6264520ed2f92c88ff4c\"\n    github_token = \"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\"\n    gist_api_url = f\"https://api.github.com/gists/{gist_id}\"\n    \n    headers = {\n        \"Authorization\": f\"token {github_token}\",\n        \"Accept\": \"application/vnd.github.v3+json\"\n    }\n\n    payload = {\n        \"files\": {\n            \"ngrok_url.json\": {\n                \"content\": json.dumps({\"ngrok_url\": url.public_url})\n            }\n        }\n    }\n\n    try:\n        response = requests.patch(gist_api_url, headers=headers, json=payload)\n        response.raise_for_status()\n        print(f\"✅ Successfully updated Gist with URL: {url.public_url}\")\n    except requests.exceptions.RequestException as e:\n        print(f\"❌ Failed to update Gist: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T23:36:01.041365Z","iopub.execute_input":"2025-06-12T23:36:01.041689Z","iopub.status.idle":"2025-06-12T23:36:01.047960Z","shell.execute_reply.started":"2025-06-12T23:36:01.041667Z","shell.execute_reply":"2025-06-12T23:36:01.047113Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Conneceting the Server of Ngrok for hosting\n# Set your actual ngrok auth token here\nngrok.set_auth_token(\"2yCOBdqsmCFkgIA5vGqo3kl9rn4_7kRvgddoeXhcH8h5g9koj\")\n\n# Open a tunnel on port 8000\npublic_url = ngrok.connect(8000)\nupdate_gist(public_url)\nprint(f\"🌐 BaseURL: {public_url.public_url}\")\nprint(f\"🌐 Public URL: {public_url.public_url}/process_document /POST\") # send doc\nprint(f\"🌐 Public URL: {public_url.public_url}/selected_titles/doc_id /POST\") # get titles\nprint(f\"🌐 Public URL: {public_url.public_url}/download/doc_id /GET\") # download slides\n\n# Run the server normally (blocking call)\nuvicorn.run(app, host=\"0.0.0.0\", port=8000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T23:36:01.049236Z","iopub.execute_input":"2025-06-12T23:36:01.049579Z"}},"outputs":[{"name":"stdout","text":"✅ Successfully updated Gist with URL: https://0463-104-155-145-222.ngrok-free.app\n🌐 BaseURL: https://0463-104-155-145-222.ngrok-free.app\n🌐 Public URL: https://0463-104-155-145-222.ngrok-free.app/process_document /POST\n🌐 Public URL: https://0463-104-155-145-222.ngrok-free.app/selected_titles/doc_id /POST\n🌐 Public URL: https://0463-104-155-145-222.ngrok-free.app/download/doc_id /GET\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [575]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"ngrok.kill()  # closes all existing tunnels before opening a new one\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T23:34:35.484818Z","iopub.execute_input":"2025-06-12T23:34:35.485502Z","iopub.status.idle":"2025-06-12T23:34:35.493815Z","shell.execute_reply.started":"2025-06-12T23:34:35.485475Z","shell.execute_reply":"2025-06-12T23:34:35.492919Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}